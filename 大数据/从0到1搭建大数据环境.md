# 从0到1搭建大数据环境

# Hadoop单机伪分布式集群搭建

## 1.准备linux环境

1. 关闭防火墙

2. 创建用户和组

   ```shell
   # hadoop缩写hdp
   useradd hadoop
   groupadd hadoop
   ```

3. 修改主机和ip的映射关系

```shell
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
10.201.100.75 cesdb hadoop
```

4.配置集群（单节点）互信

```shell
# 切换hadoop用户
su hadoop

# 生成公钥
ssh-keygen -t rsa

# 复制公钥
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
chmod 600 ~/.ssh/authorized_keys

# 重启ssh
systemctl restart sshd

# 测试
ssh localhost
ssh cesdb
ssh hadoop
```

## 2.安装jdk

将jdk的包上上传到需要的目录然后解压到相应的目录上，我的jdk目前解压到了/usr/jdk中

将java添加到环境变量中

```shell
vim /etc/profile

# JAVA_HOME
JAVA_HOME=/usr/jdk
PATH=$JAVA_HOME/bin:$PATH
CLASSPATH=:.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
export JAVA_HOME
export PATH
export CLASSPATH
# HADOOP_HOME
export HADOOP_HOME=/data/u01/app/hadoop/hadoop-3.3.6
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin

#刷新配置
source /etc/profile
```

## 3.安装hadoop - 3.3.6

1. 从官网下载[下载hadoop-3.3.6](https://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz)

2. 将文件上传到 /data/u01/soft

3. 解压文件

   ```shell
   tar -zxvf hadoop-3.3.6 -C /data/u01/app/hadoop
   ```

4. 配置hadoop文件

   第一个：hadoop-env.sh

   ```shell
   cd /data/u01/app/hadoop/hadoop-3.3.6/etc/hadoop
   
   vim hadoop-env.sh
   
   export JAVA_HOME=/usr/jdk
   export HADOOP_LOG_DIR=${HADOOP_HOME}/logs
   ```

   第二个：core-site.xml

   ```shell
   <configuration>
           <property>
                   <name>fs.defaultFS</name>
                   <value>hdfs://cesdb:8020</value>
                   <description>hdfs内部通讯访问地址</description>
           </property>
           <property>
                   <name>hadoop.tmp.dir</name>
                   <value>/data/u01/app/hadoop/hadoop-3.3.6/data/tmp</value>
           </property>
           <property>
                   <name>hadoop.proxyuser.hadoop.hosts</name>
                   <value>*</value>
           </property>
           <property>
                   <name>hadoop.proxyuser.hadoop.groups</name>
                   <value>*</value>
           </property>     
   </configuration>:
   
   ```

   第三个：hdfs-site.xml

   ```shell
   <configuration>
   
     <property>
         <name>dfs.namenode.name.dir</name>
         <value>/data/u01/app/hadoop/hadoop-3.3.6/data/namenode</value>
         <description> namenode 存放name table(fsimage)本地目录需要修改,如果没有需要自己创建文件目录)</description>
     </property>
     <property>
         <name>dfs.datanode.data.dir</name>
         <value>/data/u01/app/hadoop/hadoop-3.3.6/data/datanode</value>
         <description>datanode存放block本地目录（需要修改,如果没有需要自己创建文件目录）</description>
     </property>
     <property>
             <!--由于只有一台机器,hdfs的副本数就指定为1-->
             <name>dfs.replication</name>
             <value>1</value>
      </property>
      <property>
       <name>dfs.permissions.enabled</name>
       <value>false</value>
   </property>
   
   </configuration>
   
   ```

   第四个：mapred-site.xml

   ```shell
   <configuration>
           <!-- 指定mr运行在yarn上 -->
           <property>
                   <name>mapreduce.framework.name</name>
                   <value>yarn</value>
           </property>
   </configuration>
   ```

   第五个：yarn-site.xml

   ```shell
   <configuration>
   
   <!-- Site specific YARN configuration properties -->
   <!-- 指定YARN的老大（ResourceManager）的地址 -->
   <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>cesdb</value>
   </property>
   <!-- reducer获取数据的方式 -->
   <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
   </property>
   <!-- ResourceManager Web UI Port Configuration -->
     <property>
       <name>yarn.resourcemanager.webapp.address</name>
       <value>cesdb:8082</value>
       <description>Address where the ResourceManager web application will listen on.</description>
     </property>
   <property>
       <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
       <value>org.apache.hadoop.mapred.ShuffleHandler</value>
     </property>
     <property>
       <name>yarn.application.classpath</name>
       <value>$HADOOP_HOME/etc/hadoop:$HADOOP_HOME//share/hadoop/common/lib/*:$HADOOP_HOME//share/hadoop/common/*:$HADOOP_HOME//share/hadoop/hdfs:$HADOOP_HOME//share/hadoop/hdfs/lib/*:$HADOOP_HOME//share/hadoop/hdfs/*:$HADOOP_HOME//share/hadoop/mapreduce/lib/*:$HADOOP_HOME//share/hadoop/mapreduce/*:$HADOOP_HOME//share/hadoop/yarn:$HADOOP_HOME//share/hadoop/yarn/lib/*:$HADOOP_HOME//share/hadoop/yarn/*:/data/u01/app/hive/apache-hive-3.1.3/lib/*</value>
     </property>
   
   </configuration>
   
   
   ```

5. 第一个启动集群需要格式化namenode

   ```shell
   hdfs namenode -format
   ```

6. 启动hadoop

   ```shell
   # 先启动HDFS
   sbin/start-dfs.sh
   
   # 再启动YARN
   sbin/start-yarn.sh
   
   # jps验证是否成功
   [hadoop@cesdb logs]$ jps
   85961 Jps
   191593 DataNode
   192825 SecondaryNameNode
   225583 ResourceManager
   191372 NameNode
   225836 NodeManager
   ```

7. 访问

   ```shell
   http://10.201.100.75:9870（HDFS管理界面）
   http://10.201.100.75:8082 （MR管理界面）
   ```

# Hadoop完全分布式搭建

## 分布式集群的网络和节点规划

**参考资料**：[Hadoop完全分布式集群](https://blog.csdn.net/JunLeon/article/details/120505585)

### 网络规划

| 主机名 | IP地址        | 节点类型 |
| ------ | ------------- | -------- |
| cesdb  | 10.201.100.75 | master   |
| cesdb1 | 10.201.100.84 | slave1   |
| cesdb2 | 10.201.100.85 | slave2   |

### 节点规划

| 服务               | cesdb | cesdb1 | cesdb2 |
| ------------------ | ----- | ------ | ------ |
| NameNode           | √     |        |        |
| Secondary NameNode |       | √      |        |
| DataNode           | √     | √      | √      |
| ResourceManager    | √     |        |        |
| NodeManager        | √     | √      | √      |
| JobHistoryServer   |       |        | √      |

## 环境准备

### 配置网络映射和主机名

更改主机名

```bash
hostnamectl set-hostname <newhostname>
```

编辑网络配置文件

```bash
vim /etc/hosts

# 在文件的最后添加ip地址 主机名（三台机器都要添加）
10.201.100.75 cesdb
10.201.100.84 cesdb1
10.201.100.85 cesdb2
```

### 设置SSH无密码登录节点

这是root用户下的，如果是hadoop用户下，也需要这样操作

1.在三台机器上生成密钥文件

```bash1
ssh-keygen -t rsa
```

2.将本机公钥复制到其他机器上（三台机器都要这样操作）

```bash
ssh-copy-id cesdb
ssh-copy-id cesdb1
ssh-copy-id cesdb2
ssh-copy-id cesdb3
ssh-copy-id cesdb4
```

3.查看是否成功免密登录

```bash
ssh cesdb2
```

### Java环境

cesdb本身安装了java和hadoop，可以把相应的jdk复制到cesdb1和cesdb2中，并配置/etc/profile文件

```bash
scp -r /usr/jdk root@cesdb1:/usr
scp -r /usr/jdk root@cesdb2:/usr

# 将cesdb的/etc/profile文件内容复制给cesdb1和cesdb2
JAVA_HOME=/usr/jdk
PATH=$JAVA_HOME/bin:$PATH
CLASSPATH=:.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
export JAVA_HOME
export PATH
export CLASSPATH
# HADOOP_HOME
export HADOOP_HOME=/data/u01/app/hadoop/hadoop-3.3.6
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export HADOOP_CLASSPATH=$HADOOP_HOME/share/hadoop/mapreduce/*:$HADOOP_HOME/share/hadoop/mapreduce/lib/*
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin
# HIVE_HOME
export HIVE_HOME=/data/u01/app/hive/apache-hive-3.1.3
export PATH=$PATH:$HIVE_HOME/bin
export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$HIVE_HOME/lib/*

#刷新配置
source /etc/profile
```



## 安装配置Hadoop集群

### 先备份单机版集群数据

```bash
hdfs dfs -copyToLocal /user /data/u01/backup/hadoop_backup
```

集群搭建好后，将之前的数据备份到hadoop上

```bash
 hdfs dfs -copyFromLocal /data/u01/backup/hadoop_backup/user /
```

### 备份要修改的文件数据

因为之前的单机版hadoop是可以正常启动，所以我现把修改的配置文件先全部备份一份，以免有突发情况，我也可以恢复回去。

```bash
core-site.xml  hadoop-env.sh  hdfs-site.xml  mapred-site.xml yarn-site.xml
先把这几个文件备份起来
```

### 修改配置文件

**hadoop-env.sh**

```bash
vim hadoop-env.sh

export JAVA_HOME=/usr/jdk
export HADOOP_LOG_DIR=${HADOOP_HOME}/logs
```

**core-site.xml **

```bash
vim core-site.xml 

<configuration>
        <property>
                <name>fs.defaultFS</name>
                <value>hdfs://cesdb:8020</value>
                <description>hdfs内部通讯访问地址</description>
        </property>
        <property>
                <name>hadoop.tmp.dir</name>
                <value>/data/u01/app/hadoop/hadoop-3.3.6/data/tmp</value>
        </property>
        <property>
                <name>hadoop.proxyuser.hadoop.hosts</name>
                <value>*</value>
        </property>
        <property>
                <name>hadoop.proxyuser.hadoop.groups</name>
                <value>*</value>
        </property>
</configuration>

```

**hdfs-site.xml **

```bash
vim hdfs-site.xml 

<configuration>

 <property>
      <name>dfs.namenode.http-address</name>
      <value>cesdb:9870</value>
 </property>
 <property>
      <name>dfs.namenode.secondary.http-address</name>
      <value>cesdb1:9868</value>
</property>
  <property>
      <name>dfs.namenode.name.dir</name>
      <value>/data/u01/app/hadoop/hadoop-3.3.6/data/namenode</value>
      <description> namenode 存放name table(fsimage)本地目录需要修改,如果没有需要自己创建文件目录)</description>
  </property>
  <property>
      <name>dfs.datanode.data.dir</name>
      <value>/data/u01/app/hadoop/hadoop-3.3.6/data/datanode</value>
      <description>datanode存放block本地目录（需要修改,如果没有需要自己创建文件目录）</description>
  </property>
  <property>
          <!--由于只有一台机器,hdfs的副本数就指定为1-->
          <name>dfs.replication</name>
          <value>3</value>
   </property>
<property>
    <name>dfs.permissions.enabled</name>
    <value>false</value>
</property>
</configuration>

```

**yarn-site.xml**

```bash
vim yarn-site.xml

<configuration>

<!-- Site specific YARN configuration properties -->
<!-- 指定YARN的老大（ResourceManager）的地址 -->
<property>
     <name>yarn.resourcemanager.hostname</name>
     <value>cesdb</value>
</property>
<!-- reducer获取数据的方式 -->
<property>
     <name>yarn.nodemanager.aux-services</name>
     <value>mapreduce_shuffle</value>
</property>
<!-- ResourceManager Web UI Port Configuration -->
  <property>
    <name>yarn.resourcemanager.webapp.address</name>
    <value>cesdb:8082</value>
    <description>Address where the ResourceManager web application will listen on.</description>
  </property>
<property>
    <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
    <value>org.apache.hadoop.mapred.ShuffleHandler</value>
  </property>
  <property>
    <name>yarn.application.classpath</name>
    <value>$HADOOP_HOME/etc/hadoop:$HADOOP_HOME//share/hadoop/common/lib/*:$HADOOP_HOME//share/hadoop/common/*:$HADOOP_HOME//share/hadoop/hdfs:$HADOOP_HOME//share/hadoop/hdfs/lib/*:$HADOOP_HOME//share/hadoop/hdfs/*:$HADOOP_HOME//share/hadoop/mapreduce/lib/*:$HADOOP_HOME//share/hadoop/mapreduce/*:$HADOOP_HOME//share/hadoop/yarn:$HADOOP_HOME//share/hadoop/yarn/lib/*:$HADOOP_HOME//share/hadoop/yarn/*:/data/u01/app/hive/apache-hive-3.1.3/lib/*</value>
  </property>
 <property>
    <name>yarn.log-aggregation-enable</name>
    <value>true</value>
    <description>该参数是配置是否启用日志聚集功能</description>
 </property>
 <property>
    <name>yarn.log-aggregation.retain-seconds</name>
    <value>106800</value>
   <description>该参数是配置聚集的日志在hdfs上保存的最长时间</description>

 </property>
 <property>
    <name>yarn.nodemanager.remote-app-log-dir</name>
    <value>/user/container/logs</value>
   <description>该参数是指定日志聚合目录</description>
 </property>

</configuration>

```

**mapred-site.xml**

```bash
<configuration>
        <!-- 指定mr运行在yarn上 -->
        <property>
                <name>mapreduce.framework.name</name>
                <value>yarn</value>
        </property>
<property>
     <name>mapreduce.jobhistory.address</name>
     <value>cesdb2:10020</value>
     <description>设置mapreduce的历史服务器安装的位置及端口号</description>
 </property>
 <property>
     <name>mapreduce.jobhistory.webapp.address</name>
     <value>cesdb2:19888</value>
     <description>历史服务器的web页面地址和端口</description>
 </property>
 <property>
     <name>mapreduce.jobhistory.intermediate-done-dir</name>
     <value>${hadoop.tmp.dir}/mr-history/tmp</value>
     <description>设置存放日志文件的临时目录</description>
 </property>
 <property>
     <name>mapreduce.jobhistory.done-dir</name>
     <value>${hadoop.tmp.dir}/mr-history/done</value>
     <description>设置存放运行日志文件的最终目录</description>
 </property>
</configuration>

```



### 分发文件

在Master节点上安装及配置好hadoop系统，其他slave节点完成ssh、jdk等的安装、免密登录等，既可以将在cesdb上配置好的Hadoop分发给其他节点

```bash
scp -r /data/u01/app/hadoop root@cesdb1:/data/u01/app/
scp -r /data/u01/app/hadoop root@cesdb2:/data/u01/app/

# 在cesdb1和cesdb2中修改用户组(我的是以hadoop用户安装的，所以修改为hadoop)
chown -R hadoop:hadoop hadoop

```

### 启动集群

在启动hadoop集群前，需要先格式化NameNode，在Master主机下操作

```bash
hdfs namenode -format
```

全部启动命令：`start-all.sh`

```bash
start-all.sh        #启动HDFS和YARN
stop-all.sh         #停止HDFS和YARN
```

启动和停止历史（日志）服务器

```bash
mr-jobhistory-daemon.sh start historyserver     #启动historyserver
mr-jobhistory-daemon.sh stop historyserver     #停止historyserver
```

如果发现hadoop集群中datanode没有成功启动

解决方案：[hadoop集群启动后datanode没有启动](https://blog.csdn.net/huguihua2002/article/details/100079564)



### 时间同步

Hadoop集群对时间要求非常高，主节点与各从节点的时间都必须同步。NTP使用来使计算机时间同步的一种协议。配置时间同步服务器（NTP服务器）主要就是为了进行集群的时间同步。RHEL7操作系统默认安装有chrony时间同步服务，直接修改相关配置文件即可使用chrony服务做时间同步。

检查当前时区

```bash
timedatectl

# 如果不一致，设置为一致的
timedatectl set-timezone Asia/Shanghai
```

修改chrony配置文件

```bash
vim /etc/chrony.conf

server 199.168.200.120 iburst
```

重启时间同步服务

```bash
systemctl enable chronyd.service 
systemctl restart chronyd.service 
systemctl status chronyd.service 
```

查看时间同步源

```bash
chronyc sources -v
```

显示时间同步源

```bash
chronyc sourcestats 
```

测试集群间的时间同步

```bash
date '+%Y-%m-%d %H:%M:%S'
```

## 基于Zookeeper的Hadoop HA集群高可用

**参考资料：**

[Hadoop HA高可用](https://blog.csdn.net/JunLeon/article/details/120689889?spm=1001.2014.3001.5502)

[基于ZooKeeper搭建Hadoop高可用集群](https://www.bookstack.cn/read/BigData-Notes/notes-installation-%E5%9F%BA%E4%BA%8EZookeeper%E6%90%AD%E5%BB%BAHadoop%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4.md)

# Hadoop集群扩展

## 1.先配置好hostname以及搞定ssh的问题

## 2.配置好cesdb（主节点）上的文件

[core-site.xml](./file/hadoop/core-site.xml)

[hdfs-site.xml](./file/hadoop/hdfs-site.xml)

[yarn-site.xml](./file/hadoop/yarn-site.xml)

[mapred-site.xml](./file/hadoop/mapred-site.xml)

[capacity-scheduler.xml](./file/hadoop/capacity-scheduler.xml)

[workers](./file/hadoop/workers)

## 3.分发节点信息

```bash
sync_script.sh /data/u01/app/hadoop/hadoop-3.3.6/etc/hadoop/
```

## 4.重启集群

## 5.刷新节点信息

```bash
# 在现有的 NameNode 上，运行以下命令来刷新节点信息，使其识别新加入的节点。
hdfs dfsadmin -refreshNodes

# 确认集群节点是否加入集群
hdfs dfsadmin -report

```

## 6.查看数据分布

```bash
hdfs dfs -du -h /
```

## 7.执行负载均衡（可选）

```bash
hdfs balancer
```

## 8.问题

成功配置后，也成功启动了，但是hadoop web界面却还是显示3个节点。另外两个节点并没有显示出来。这是因为cesdb3和cesdb4的机器是克隆cesdb1的，hadoop里面的datanode版本号都一样，所以只会显示一个。

```bash
--cesdb--
[hadoop@cesdb current]$ cat VERSION 
#Tue Dec 10 14:29:00 CST 2024
storageID=DS-d36f5720-8aeb-433c-97b0-4bb240db1160
clusterID=CID-66eefb78-70e9-4ba5-b3f9-aea376462c51
cTime=0
datanodeUuid=2d72145b-0aec-451a-bba7-aa9f67c7df43
storageType=DATA_NODE
layoutVersion=-57

--cesdb1--
[hadoop@cesdb1 current]$ cat VERSION 
#Tue Dec 10 14:29:00 CST 2024
storageID=DS-21f0d56f-95f9-4449-99c5-fd640dbaddd4
clusterID=CID-66eefb78-70e9-4ba5-b3f9-aea376462c51
cTime=0
datanodeUuid=e7342a5e-b53d-43e8-bd48-e2d9436356b8
storageType=DATA_NODE
layoutVersion=-57
[hadoop@cesdb1 current]$

--cesdb2--
[hadoop@cesdb2 current]$ cat VERSION 
#Tue Dec 10 14:28:59 CST 2024
storageID=DS-94220399-81d6-4ebe-a4d3-5d6736190d73
clusterID=CID-66eefb78-70e9-4ba5-b3f9-aea376462c51
cTime=0
datanodeUuid=6e8402ad-ffdb-4f8d-82f6-7c64eb4815a2
storageType=DATA_NODE
layoutVersion=-57
[hadoop@cesdb2 current]$ 

--cesdb3--
[hadoop@cesdb3 current]$ cat VERSION 
#Tue Dec 10 14:28:59 CST 2024
storageID=DS-21f0d56f-95f9-4449-99c5-fd640dbaddd4
clusterID=CID-66eefb78-70e9-4ba5-b3f9-aea376462c51
cTime=0
datanodeUuid=e7342a5e-b53d-43e8-bd48-e2d9436356b8
storageType=DATA_NODE
layoutVersion=-57
[hadoop@cesdb3 current]$ 

--cesdb4--
[hadoop@cesdb4 current]$ cat VERSION 
#Tue Dec 10 14:28:59 CST 2024
storageID=DS-21f0d56f-95f9-4449-99c5-fd640dbaddd4
clusterID=CID-66eefb78-70e9-4ba5-b3f9-aea376462c51
cTime=0
datanodeUuid=e7342a5e-b53d-43e8-bd48-e2d9436356b8
storageType=DATA_NODE
layoutVersion=-57
```

解决方案：

把cesdb3和cesdb4的datanode存储目录删除掉，重启集群让他重新生成

```bash
rm -rf /data/u01/app/hadoop/hadoop-3.3.6/data/datanode/*
```



# Hive搭建

在已经搭建好的 Hadoop 环境中安装和配置 Hive，并使用 Oracle 作为元数据存储，需要进行以下步骤：

## 1. 下载和安装 Hive
首先，从 Apache 官网下载 Hive，并进行安装：

```bash
# 下载 Hive
wget https://downloads.apache.org/hive/hive-3.1.3/apache-hive-3.1.3-bin.tar.gz

# 解压缩 Hive
tar -xzvf apache-hive-3.1.3-bin.tar.gz -C /data/u01/soft/hive
```

## 2. 配置环境变量
配置 Hive 的环境变量：

```bash
vim /etc/profile
```

在文件末尾添加以下内容：

```bash
# Hive environment variables
export HIVE_HOME=/data/u01/app/hive/apache-hive-3.1.3
export PATH=$PATH:$HIVE_HOME/bin
```

使环境变量生效：

```bash
source /etc/profile
```

## 3. 下载并配置 Oracle JDBC 驱动
从 Oracle 官网下载 Oracle JDBC 驱动（例如 `ojdbc8.jar`），然后将其复制到 Hive 的 lib 目录：

```bash
cp /path/to/ojdbc8.jar $HIVE_HOME/lib/
```

## 4. 配置 Hive 使用 Oracle 作为元数据存储

创建一个日志文件夹

```shell
mkdir logs
```

进入配置文件夹

```shell
cd /data/u01/app/hive/apache-hive-3.1.3/conf

touch hive-site.xml
cp hive-env.sh.template hive-env.sh
# 不手动添加的话,hive不打印日志！！！
cp hive-log4j2.properties.template hive-log4j2.properties
cp hive-exec-log4j2.properties.template hive-exec-log4j2.properties
```

修改hive-env.sh

```shell
vim hive-env.sh

HADOOP_HOME=/data/u01/app/hadoop/hadoop-3.3.6
```

编辑 Hive 的配置文件 `hive-site.xml`：

```bash
vim hive-site.xml
```

添加以下配置：

```xml
<configuration>
    
    <property>
  <name>fs.defaultFS</name>
  <value>hdfs://cesdb:8020</value>
</property>
   <!--
	//oracle服务的方式
	jdbc:oracle:thin:@//199.168.200.55:1521/docare

	//oracle sid的方法是
	jdbc:oracle:thin:@10.201.100.75:1521:cesdb
	--> 
    
  <!-- Oracle JDBC connection string -->
  <property>
    <name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:oracle:thin:@10.201.100.75:1521:cesdb</value>
    <description>JDBC connect string for a JDBC metastore</description>
  </property>

  <property>
    <name>javax.jdo.option.ConnectionDriverName</name>
    <value>oracle.jdbc.OracleDriver</value>
    <description>Driver class name for a JDBC metastore</description>
  </property>

  <property>
    <name>javax.jdo.option.ConnectionUserName</name>
    <value>用户名</value>
    <description>Username to use against metastore database</description>
  </property>

  <property>
    <name>javax.jdo.option.ConnectionPassword</name>
    <value>密码</value>
    <description>Password to use against metastore database</description>
  </property>


  <!-- Hive Execution Configuration -->
  <property>
    <name>hive.execution.engine</name>
    <value>mr</value>
    <description>Execution engine to use. mr is mapreduce, tez is Tez, spark is Spark</description>
  </property>
    
    <property>
        <!--hive表在hdfs的位置-->
        <name>hive.metastore.warehouse.dir</name>
        <value>/user/hive/warehouse</value>
    </property>
    <property>
        <name>hive.security.authorization.enabled</name>
        <value>false</value>
    </property>
    <property>
        <name>hive.security.authorization.createtable.owner.grants</name>
        <value>ALL</value>
    </property>
    <property>
        <name>hive.server2.enable.doAs</name>
        <value>false</value>
    </property>
    <property>
  		<name>hive.metastore.partition.management.task.frequency</name>
  		<value>1d</value>
 		 <description>Frequency at which the partition management task runs to add partitions.</description>
	</property>

</configuration>
```

## 5. 初始化 Hive Metastore
先给Hadoop配置init-env.sh

```shell
cd /data/u01/app/hadoop/hadoop-3.3.6/etc/hadoop

touch init-env.sh

vim init-env.sh

#!/bin/bash
# 添加新的环境变量
export HADOOP_HOME=/data/u01/app/hadoop/hadoop-3.3.6
export HADOOP_CONF_DIR=$HADOOP_HOME/conf
export HADOOP_LOG_DIR=/data/u01/app/hadoop/hadoop-3.3.6/logs
export PATH=$PATH:$HADOOP_HOME/bin

source init-env.sh
```

给Hive配置init-env.sh

```shell
cd /data/u01/app/hive/apache-hive-3.1.3/conf

touch init-env.sh

vim init-env.sh

#!/bin/bash
export HIVE_HOME=/data/u01/app/hive/apache-hive-3.1.3
export HIVE_CONF_DIR=$HIVE_HOME/conf
export PATH=$PATH:$HIVE_HOME/bin

source init-env.sh
```

配置hive启动脚本

启动hive，注：`一定要确保hadoop已经成功启动，才能启动hive，否则连接hive beeline会卡死但是不报错！！！`

```shell
cd /data/u01/app/hive/apache-hive-3.1.3/bin

touch start-all.sh

#!/bin/bash
nohup $HIVE_HOME/bin/hive --service metastore &
nohup $HIVE_HOME/bin/hive --service hiveserver2 &

chmod -R 777 start-all.sh 
```

配置hive停止脚本

```shell
touch stop-all.sh

jps | grep RunJar | awk '{print $1}' | xargs kill -9

chmod -R 777 stop-all.sh 
```

你需要使用 `schematool` 来初始化 Hive Metastore 的 schema：

```bash
schematool -dbType oracle -initSchema
```

## 6. 启动 Hive
现在你可以启动 Hive 并进入 Hive 命令行界面：

```bash
sh start-all.sh
```

判断linux端口使用是否已经监听

```shell
[hadoop@cesdb bin]$ netstat -ntulp |grep 9083
(Not all processes could be identified, non-owned process info
 will not be shown, you would have to be root to see it all.)
tcp6       0      0 :::9083                 :::*                    LISTEN      120633/java         
[hadoop@cesdb bin]$ ps -ef | grep 120633
hadoop   120633      1  8 09:35 pts/11   00:00:15 /usr/jdk/bin/java -Dproc_jar -Dproc_metastore -Dlog4j2.formatMsgNoLookups=true -Dlog4j.configurationFile=hive-log4j2.properties -Djava.util.logging.config.file=/data/u01/app/hive/apache-hive-3.1.3/conf/parquet-logging.properties -Dyarn.log.dir=/data/u01/app/hadoop/hadoop-3.3.6/logs -Dyarn.log.file=hadoop.log -Dyarn.home.dir=/data/u01/app/hadoop/hadoop-3.3.6 -Dyarn.root.logger=INFO,console -Djava.library.path=/data/u01/app/hadoop/hadoop-3.3.6/lib/native -Xmx256m -Dhadoop.log.dir=/data/u01/app/hadoop/hadoop-3.3.6/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/data/u01/app/hadoop/hadoop-3.3.6 -Dhadoop.id.str=hadoop -Dhadoop.root.logger=INFO,console -Dhadoop.policy.file=hadoop-policy.xml -Dhadoop.security.logger=INFO,NullAppender org.apache.hadoop.util.RunJar /data/u01/app/hive/apache-hive-3.1.3/lib/hive-metastore-3.1.3.jar org.apache.hadoop.hive.metastore.HiveMetaStore
hadoop   128822 191526  0 09:37 pts/11   00:00:00 grep --color=auto 120633
[hadoop@cesdb bin]$ netstat -ntulp | grep 10000
(Not all processes could be identified, non-owned process info
 will not be shown, you would have to be root to see it all.)
tcp6       0      0 :::10000                :::*                    LISTEN      120634/java         
[hadoop@cesdb bin]$ ps -ef | grep 120634
hadoop   120634      1 11 09:35 pts/11   00:00:27 /usr/jdk/bin/java -Dproc_jar -Dproc_hiveserver2 -Dlog4j2.formatMsgNoLookups=true -Dlog4j.configurationFile=hive-log4j2.properties -Djava.util.logging.config.file=/data/u01/app/hive/apache-hive-3.1.3/conf/parquet-logging.properties -Djline.terminal=jline.UnsupportedTerminal -Dyarn.log.dir=/data/u01/app/hadoop/hadoop-3.3.6/logs -Dyarn.log.file=hadoop.log -Dyarn.home.dir=/data/u01/app/hadoop/hadoop-3.3.6 -Dyarn.root.logger=INFO,console -Djava.library.path=/data/u01/app/hadoop/hadoop-3.3.6/lib/native -Xmx256m -Dhadoop.log.dir=/data/u01/app/hadoop/hadoop-3.3.6/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/data/u01/app/hadoop/hadoop-3.3.6 -Dhadoop.id.str=hadoop -Dhadoop.root.logger=INFO,console -Dhadoop.policy.file=hadoop-policy.xml -Dhadoop.security.logger=INFO,NullAppender org.apache.hadoop.util.RunJar /data/u01/app/hive/apache-hive-3.1.3/lib/hive-service-3.1.3.jar org.apache.hive.service.server.HiveServer2
hadoop   131711 191526  0 09:39 pts/11   00:00:00 grep --color=auto 120634

```



## 7. 测试 Hive

在 Hive 命令行中运行一些基本命令来验证 Hive 的安装：

这是第一台客户端（已经不用了）

```sql
hive

CREATE TABLE test_table (id INT, name STRING);

SHOW TABLES;

INSERT INTO test_table (id, name) VALUES (1, 'Alice'), (2, 'Bob');

select * from test_table;
```

如果这些命令成功执行并返回结果，说明 Hive 安装和配置成功。

第二代客户端 beeline

HiveServer2通过Metastore服务读写元数据。所以在远程模式下，启动HiveServer2之前必须先启动metastore服务。

远程模式下，Beeline客户端只能通过HiveServer2服务访问Hive，而bin/hive是通过Metastore服务访问的，关系如下：

![](D:\Github\MyKnowledgeRepository\img\DW_img\Hive客户端和服务的关系.png)

```shell
cd $HIVE_HOME/bin

beeline
!connect jdbc:hive2://主机名:10000
然后输入用户名和密码

beeline> !connect jdbc:hive2://cesdb:10000
Connecting to jdbc:hive2://cesdb:10000
Enter username for jdbc:hive2://cesdb:10000: hadoop
Enter password for jdbc:hive2://cesdb:10000: ******
Connected to: Apache Hive (version 3.1.3)
Driver: Hive JDBC (version 3.1.3)
Transaction isolation: TRANSACTION_REPEATABLE_READ
0: jdbc:hive2://cesdb:10000> select * from test_table;
INFO  : Compiling command(queryId=hadoop_20240612155511_de96a975-0b0b-4766-8b7e-d9ce68587494): select * from test_table
INFO  : Concurrency mode is disabled, not creating a lock manager
INFO  : Semantic Analysis Completed (retrial = false)
INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:test_table.id, type:int, comment:null), FieldSchema(name:test_table.name, type:string, comment:null)], properties:null)
INFO  : Completed compiling command(queryId=hadoop_20240612155511_de96a975-0b0b-4766-8b7e-d9ce68587494); Time taken: 3.078 seconds
INFO  : Concurrency mode is disabled, not creating a lock manager
INFO  : Executing command(queryId=hadoop_20240612155511_de96a975-0b0b-4766-8b7e-d9ce68587494): select * from test_table
INFO  : Completed executing command(queryId=hadoop_20240612155511_de96a975-0b0b-4766-8b7e-d9ce68587494); Time taken: 0.006 seconds
INFO  : OK
INFO  : Concurrency mode is disabled, not creating a lock manager
+----------------+------------------+
| test_table.id  | test_table.name  |
+----------------+------------------+
| 1              | Alice            |
| 2              | Bob              |
+----------------+------------------+
2 rows selected (3.869 seconds)
```



# Hive集群搭建

1.将配置好的单机Hive即在cesdb节点上的hive分发给cesdb1和cesdb2

```bash
# 要用root用户
scp -r /data/u01/app/hive root@cesdb1:/data/u01/app/
scp -r /data/u01/app/hive root@cesdb2:/data/u01/app/

# 修改目录权限
chown -R hadoop:hadoop hive
```

2.cesdb需要添加上这个配置，cesdb1和cesdb2修改配置hive-site.xml

```bash
<property>
  <name>fs.defaultFS</name>
  <value>hdfs://cesdb:8020</value>
</property>

 <!-- Hive Metastore Thrift URI -->
  <property>
    <name>hive.metastore.uris</name>
    <value>thrift://cesdb:9083</value>
  </property>
```

3.进入cesdb，进入命令行，启动hive（不需要初始化元数据了，不然之前的数据全没了，只需要在元数据那台机器启动即可）

```bash
sh start-all.sh
```

# Hive on Spark配置

参考链接：[Hive on Spark配置](https://blog.csdn.net/weixin_46389691/article/details/134126254)

## 1.上传jar包并解压

```bash
tar -xvf spark-2.3.0-bin-without-hadoop.tgz -C /data/u01/app/spark/

cd /data/u01/app/spark
mv spark-2.3.0-bin-without-hadoop/ spark-2.3.0
```

## 2.配置环境变量

```bash
vim /etc/profile

#SPAKR_HOME
export SPARK_HOME=/data/u01/app/spark/spark-2.3.0
export PATH=$PATH:$SPARK_HOME/bin

# 刷新环境变量
source /etc/profile
```

## 3.配置spark参数

指定hadoop路径

```bash
cd $SPARK_HOME/conf

mv spark-env.sh.template spark-env.sh

vim spark-env.sh

export SPARK_DIST_CLASSPATH=$(hadoop classpath)
```

配置spark参数

```bash
cd $HIVE_HOME/conf

vim spark-default.conf

# 指定提交到 yarn 运行
spark.master                             yarn
# 开启日志并存储到 HDFS 上
spark.eventLog.enabled                   true
spark.eventLog.dir                       hdfs://cesdb:8020/user/spark/spark-logs
# 指定每个执行器的内存
spark.executor.memory                    4g
# 指定每个调度器的内存
spark.driver.memory					     2g
spark.executor.cores                     2

cd $SPARK_HOME/conf
vim spark-defaults.conf

spark.master                             yarn
spark.eventLog.enabled                   true
spark.eventLog.dir                       hdfs://cesdb:8020/user/spark/spark-logs
# Executor 配置
spark.executor.memory                    8g            # 每个 executor 分配 8GB 内存
spark.executor.cores                     4             # 每个 executor 使用 4 核心
spark.executor.instances                 70            # 启动 70 个 executor
# Driver 配置
spark.driver.memory                      4g            # Driver 分配 4GB 内存
# 动态资源分配配置（可选）
spark.dynamicAllocation.enabled          true          # 启用动态资源分配
spark.dynamicAllocation.minExecutors     2            # 最小 executor 数量
spark.dynamicAllocation.maxExecutors     70           # 最大 executor 数量
# SQL shuffle 分区配置
spark.sql.shuffle.partitions             200           # 增加 shuffle 分区数以优化 shuffle 性能
spark.sql.adaptive.enabled               true          # 开启自适应查询执行，Spark 会根据作业的实际执行情况动态调整查询执行策略。

```

先前配置

```
spark.master                             yarn
spark.eventLog.enabled                   true
spark.eventLog.dir                       hdfs://cesdb:8020/user/spark/spark-logs
spark.executor.memory                    14g
spark.driver.memory                      12g
spark.yarn.executor.memoryOverhead       2g
spark.driver.memoryOverhead              2g
spark.executor.cores                     8
spark.dynamicAllocation.enabled          true
spark.shuffle.service.enabled            true
spark.dynamicAllocation.initialExecutors 1
spark.dynamicAllocation.minExecutors     1
spark.dynamicAllocation.maxExecutors     35
spark.dynamicAllocation.executorIdleTimeout 60s
spark.dynamicAllocation.schedulerBacklogTimeout 3s

```

调整后

```bash
spark.master                             yarn
spark.eventLog.enabled                   true
spark.eventLog.dir                       hdfs://cesdb:8020/user/spark/spark-logs
spark.executor.memory                    14g
spark.driver.memory                      12g
spark.yarn.executor.memoryOverhead       2g
spark.driver.memoryOverhead              2g
spark.executor.cores                     4 # 调整为4核
spark.dynamicAllocation.enabled          true
spark.shuffle.service.enabled            true
spark.dynamicAllocation.initialExecutors 1
spark.dynamicAllocation.minExecutors     1
spark.dynamicAllocation.maxExecutors     70 # 56/4 * 5 = 70 提高了70核 
spark.dynamicAllocation.executorIdleTimeout 60s
spark.dynamicAllocation.schedulerBacklogTimeout 3s


spark.master = yarn
spark.eventLog.enabled = true
spark.eventLog.dir = hdfs://cesdb:8020/user/spark/spark-logs

# Executor设置
spark.executor.memory = 10g
spark.executor.cores = 4
spark.executor.memoryOverhead = 2g

# Driver设置
spark.driver.memory = 10g
spark.driver.memoryOverhead = 2g

# 动态资源分配
spark.dynamicAllocation.enabled = true
spark.dynamicAllocation.initialExecutors = 2
spark.dynamicAllocation.minExecutors = 1
spark.dynamicAllocation.maxExecutors = 25
spark.dynamicAllocation.executorIdleTimeout = 60s
spark.dynamicAllocation.schedulerBacklogTimeout = 3s

# Shuffle设置
spark.shuffle.service.enabled = true
spark.sql.shuffle.partitions = 100

```



配置好后，在hadoop上创建以下两个存储目录

```bash
hadoop fs -mkdir -p /user/spark/spark-logs
hadoop fs -mkdir -p /user/spark/spark-jars
```

拷贝hive jar包到spark中

```bash
cp $HIVE_HOME/lib/hive-exec-3.1.3.jar $SPARK_HOME/jars/
```

上传jar包并更换引擎。

因为只在一台机器上安装了 Hive 和 Spark，所以当我们将任务提交到 Yarn 上进行调度时，可能会将该任务分配到其它节点，这就会导致任务无法正常运行，所以我们需要将 Spark 中的所有 Jar 包到 HDFS 上，并告知 Hive 其存储的位置。

```bash
cd $SPARK_HOME

hadoop fs -put ./jars/* /user/spark/spark-jars
```

## 4.编辑hive-site.xml文件

```bash
cd $HIVE_HOME/conf

vim hive-site.xml

<!--Spark依赖位置-->
<property>
    <name>spark.yarn.jars</name>
    <value>hdfs://cesdb:8020/user/spark/spark-jars/*</value>
</property>
  
<!--Hive执行引擎-->
<property>
    <name>hive.execution.engine</name>
    <value>spark</value>
</property>

<!--提交任务超时时间，单位ms-->
<property>
    <name>hive.spark.client.connect.timeout</name>
    <value>10000</value>
</property>

```

## 5.测试

先重启hive

进入hive中创建测试表：

```bash
drop table if exists books;
create table books(id int,book_name string);

insert into books values (1,'bigdata');
insert into books values (2,'hive');
insert into books values (3,'spark');

```

## 6.报错处理

1.依赖冲突。当我们在使用 Hive On Spark 时，可能会发生如下依赖冲突问题：

```bash
ERROR : Job failed with java.lang.IllegalAccessError: tried to access method com.google.common.base.Stopwatch.<init>()V from class org.apache.hadoop.mapreduce.lib.input.FileInputFormat
        at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:262)
        at org.apache.hadoop.hive.shims.Hadoop23Shims$1.listStatus(Hadoop23Shims.java:134)
        at org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat.getSplits(CombineFileInputFormat.java:217)
        at org.apache.hadoop.mapred.lib.CombineFileInputFormat.getSplits(CombineFileInputFormat.java:75)
        at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileInputFormatShim.getSplits(HadoopShimsSecure.java:321)
        at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getCombineSplits(CombineHiveInputFormat.java:444)
        at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getSplits(CombineHiveInputFormat.java:564)
        at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:200)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)
        at scala.Option.getOrElse(Option.scala:121)
        at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
        at org.apache.spark.rdd.RDD.getNumPartitions(RDD.scala:267)
        at org.apache.spark.api.java.JavaRDDLike$class.getNumPartitions(JavaRDDLike.scala:65)
        at org.apache.spark.api.java.AbstractJavaRDDLike.getNumPartitions(JavaRDDLike.scala:45)
        at org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.generateMapInput(SparkPlanGenerator.java:215)
        at org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.generateParentTran(SparkPlanGenerator.java:142)
        at org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.generate(SparkPlanGenerator.java:114)
        at org.apache.hadoop.hive.ql.exec.spark.RemoteHiveSparkClient$JobStatusJob.call(RemoteHiveSparkClient.java:359)
        at org.apache.hive.spark.client.RemoteDriver$JobWrapper.call(RemoteDriver.java:378)
        at org.apache.hive.spark.client.RemoteDriver$JobWrapper.call(RemoteDriver.java:343)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)

```

这是由于 Hive 中的 `guava` 包版本比较高，与 Spark 不太兼容，所以我们需要更换为低一点的版本，建议使用 `guava-13.0.jar` 版本。

Jar 包获取地址：[guava-13.0.jar](https://repo1.maven.org/maven2/com/google/guava/guava/13.0/)

```bash
# 备份 Hive 的高版本
cd $HIVE_HOME/lib
mv guava-19.0.jar guava-19.0.jar.bak

# 将低版本放入 Hive 与 Spark 中
cp guava-13.0.jar $HIVE_HOME/lib
cp guava-13.0.jar $SPARK_HOME/jars

# 还需上传到 HDFS 中存储 Spark Jars 的目录下
hadoop fs -put guava-13.0.jar /user/spark/spark-jars

```

重新启动 Hive 终端就可以生效啦。



2.java.lang.NoSuchMethodError: org.apache.orc.OrcFile 问题

参考链接：[Hive on spark 配置问题解决](https://www.jianshu.com/p/8c48adb2a31a)

这是由于spark2.3.0和hive 3.1.3版本的jar包不兼容引起的，产生了jar包冲突。

解决办法：删掉hdfs上的orc-core-1.4.4-nohive.jar即可（前提是已经配置了hive-site配置了spark.yarn.jars）

```bash
[hadoop@cesdb jars]$ hdfs dfs -ls -R /user/spark/spark-jars |grep "orc-core-1.4.1-nohive.jar"
-rw-r--r--   3 hadoop supergroup    1441994 2025-01-08 17:22 /user/spark/spark-jars/orc-core-1.4.1-nohive.jar
[hadoop@cesdb jars]$ hadoop fs -rm /user/spark/spark-jars/orc-core-1.4.1-nohive.jar
Deleted /user/spark/spark-jars/orc-core-1.4.1-nohive.jar
[hadoop@cesdb jars]$ hdfs dfs -ls -R /user/spark/spark-jars |grep "orc-core-1.4.1-nohive.jar"
[hadoop@cesdb jars]$ 
```

3.跟问题2一样，找不到支持parquet格式的方法。这是由于hive和spark版本不兼容导致的。找到spark中相应的parquet包，把他们删除。

parquet-column-1.8.2.jar
parquet-common-1.8.2.jar
parquet-encoding-1.8.2.jar
parquet-format-2.3.1.jar
parquet-hadoop-1.8.2.jar

```bash
[hadoop@cesdb jars]$ ls |grep  'parquet-*'
parquet-column-1.8.2.jar
parquet-common-1.8.2.jar
parquet-encoding-1.8.2.jar
parquet-format-2.3.1.jar
parquet-hadoop-1.8.2.jar
parquet-jackson-1.8.2.jar
[hadoop@cesdb jars]$ hdfs dfs -ls -R /user/spark/spark-jars |grep "parquet-hadoop-1.8.2.jar"
-rw-r--r--   3 hadoop supergroup     250377 2025-01-08 17:22 /user/spark/spark-jars/parquet-hadoop-1.8.2.jar
[hadoop@cesdb jars]$ hdfs dfs -rm /user/spark/spark-jars/parquet-hadoop-1.8.2.jar
Deleted /user/spark/spark-jars/parquet-hadoop-1.8.2.jar
[hadoop@cesdb jars]$ hdfs dfs -ls -R /user/spark/spark-jars |grep "parquet-hadoop-1.8.2.jar"
[hadoop@cesdb jars]$ hdfs dfs -ls -R /user/spark/spark-jars |grep "parquet-column-1.8.2.jar"
-rw-r--r--   3 hadoop supergroup     956984 2025-01-08 17:22 /user/spark/spark-jars/parquet-column-1.8.2.jar
[hadoop@cesdb jars]$ hdfs dfs -rm /user/spark/spark-jars/parquet-column-1.8.2.jar
Deleted /user/spark/spark-jars/parquet-column-1.8.2.jar
[hadoop@cesdb jars]$ hdfs dfs -ls -R /user/spark/spark-jars |grep "parquet-column-1.8.2.jar"
[hadoop@cesdb jars]$ hdfs dfs -ls -R /user/spark/spark-jars |grep "parquet-common-1.8.2.jar"
-rw-r--r--   3 hadoop supergroup      42032 2025-01-08 17:22 /user/spark/spark-jars/parquet-common-1.8.2.jar
[hadoop@cesdb jars]$ hdfs dfs -rm /user/spark/spark-jars/parquet-common-1.8.2.jar
Deleted /user/spark/spark-jars/parquet-common-1.8.2.jar
[hadoop@cesdb jars]$ hdfs dfs -rm /user/spark/spark-jars/parquet-encoding-1.8.2.jar
Deleted /user/spark/spark-jars/parquet-encoding-1.8.2.jar
[hadoop@cesdb jars]$ hdfs dfs -rm /user/spark/spark-jars/parquet-format-2.3.1.jar
Deleted /user/spark/spark-jars/parquet-format-2.3.1.jar
[hadoop@cesdb jars]$ 

```



## 7.sprak历史服务器配置

1.配置spark-defaults.conf，添加以下配置

```bash
spark.eventLog.enabled                   true
spark.eventLog.dir                       hdfs://cesdb:8020/user/spark/spark-logs
spark.history.fs.logDirectory            hdfs://cesdb:8020/user/spark/spark-logs
spark.history.ui.port                    18080
spark.yarn.historyServer.address         http://cesdb:18080
```

配置好后，一定要把这个文件复制到hive配置文件下。如果不做这一步操作，将会导致yarn的tracking url不能够正确跳转到spark历史服务器

```bash
[hadoop@cesdb conf]$ cp spark-defaults.conf /data/u01/app/hive/apache-hive-3.1.3/conf/
```

2.配置spark-env.sh，添加以下配置

```bash
export SPARK_HISTORY_OPTS="-Dspark.history.ui.port=18080 -Dspark.history.fs.logDirectory=hdfs://cesdb:8020/user/spark/spark-logs"
```

3.启动spark history server

```bash
$SPARK_HOME/sbin/start-history-server.sh

# jps命令查看是否启动，HistoryServer服务就是spark的
[hadoop@cesdb conf]$ jps
139126 HistoryServer
55237 Jps
71351 NodeManager
71128 ResourceManager
216191 DataNode
124365 RunJar
215932 NameNode
124366 RunJar
[hadoop@cesdb conf]$ 

```

4.访问以下链接，看是否能否打开

```bash
http://cesdb:18080/
```

5.日志清理（可选）

为了防止日志过多占用Hdfs，可以设置自动清理

```bash
spark.history.retainedApplications    50
spark.history.fs.cleaner.enabled     true
spark.history.fs.cleaner.interval    1d
spark.history.fs.cleaner.maxAge      7d
```

6.注意事项

但凡修改了spark-defaults.conf文件，一定要拷贝一份到hive配置文件目录下，这样才会生效。

最终的spark-defaults.conf文件如下

```bash
spark.master                             yarn
spark.eventLog.enabled                   true
spark.eventLog.dir                       hdfs://cesdb:8020/user/spark/spark-logs
spark.history.fs.logDirectory            hdfs://cesdb:8020/user/spark/spark-logs
spark.history.ui.port                    18080
spark.yarn.historyServer.address         http://cesdb:18080
spark.executor.memory                    14g
spark.driver.memory                      12g
spark.yarn.executor.memoryOverhead       2g
spark.driver.memoryOverhead              2g
spark.executor.cores                     8
spark.dynamicAllocation.enabled          true
spark.shuffle.service.enabled            true
spark.dynamicAllocation.initialExecutors 1
spark.dynamicAllocation.minExecutors     1
spark.dynamicAllocation.maxExecutors     35
spark.dynamicAllocation.executorIdleTimeout 60s
spark.dynamicAllocation.schedulerBacklogTimeout 3s
spark.history.retainedApplications    50
spark.history.fs.cleaner.enabled     true
spark.history.fs.cleaner.interval    1d
spark.history.fs.cleaner.maxAge      7d
```

## Hive on Spark和Spark on hive的区别

**Hive on Spark** 和 **Spark on Hive** 是两种不同的集成方式，它们的核心区别在于**主导框架**和**使用场景**。

---

### 1. **Hive on Spark**
   - **定义**：
     - Hive on Spark 是指 Hive 使用 Spark 作为其执行引擎。
     - Hive 仍然是主导框架，负责 SQL 解析、查询优化和元数据管理，而 Spark 仅作为计算引擎来执行任务。
   - **架构**：
     - Hive 将 SQL 查询解析为逻辑计划，并将其转换为 Spark 的物理执行计划。
     - Spark 负责执行任务，并将结果返回给 Hive。
   - **使用场景**：
     - 适合已有 Hive 生态的场景，尤其是需要兼容 Hive SQL 语法的批处理任务。
     - 适合数据仓库、ETL 等传统大数据处理场景。
   - **优点**：
     - 兼容 Hive SQL，迁移成本低。
     - 适合已有 Hive 生态系统的团队。
   - **缺点**：
     - 性能受限于 Hive 的查询优化能力。
     - 对低延迟查询和复杂计算支持有限。

---

### 2. **Spark on Hive**
   - **定义**：
     - Spark on Hive 是指 Spark 直接访问 Hive 的元数据（Hive Metastore）和数据存储（如 HDFS）。
     - Spark 是主导框架，Hive 仅作为元数据管理和数据存储的辅助工具。
   - **架构**：
     - Spark 直接读取 Hive 表的数据，并使用自己的计算引擎（如 Spark SQL、DataFrame API）进行处理。
     - Hive 仅提供元数据（表结构、分区信息等）和数据存储支持。
   - **使用场景**：
     - 适合需要高性能计算、低延迟查询和复杂数据处理的场景。
     - 适合实时分析、交互式查询、流处理和机器学习等场景。
   - **优点**：
     - 性能更高，尤其是对复杂计算和低延迟查询。
     - 支持多种数据源和计算模式（如流处理、机器学习）。
   - **缺点**：
     - 需要独立部署 Spark 集群，运维复杂度较高。
     - 对 Hive SQL 的兼容性有限（部分 Hive 语法可能不支持）。

---

### 3. **核心区别**
| 特性           | Hive on Spark                 | Spark on Hive                         |
| -------------- | ----------------------------- | ------------------------------------- |
| **主导框架**   | Hive 主导，Spark 作为执行引擎 | Spark 主导，Hive 提供元数据和存储支持 |
| **查询优化**   | 依赖 Hive 的优化器            | 依赖 Spark 的 Catalyst 优化器         |
| **性能**       | 适合批处理，延迟较高          | 高性能，适合低延迟和复杂计算          |
| **使用场景**   | 批处理、ETL、数据仓库         | 实时分析、交互查询、流处理、机器学习  |
| **SQL 兼容性** | 完全兼容 Hive SQL             | 部分兼容 Hive SQL                     |
| **资源管理**   | 依赖 Hive 和 YARN             | 灵活（Standalone、YARN、K8s 等）      |
| **运维复杂度** | 较低                          | 较高                                  |
| **灵活性**     | 较低，受限于 Hive             | 高，支持多种数据源和 API              |

---

### 4. **如何选择？**
- **选择 Hive on Spark**：
  - 如果你已经有一个成熟的 Hive 生态系统。
  - 如果你的主要需求是批处理任务和 ETL。
  - 如果你希望最小化迁移成本，继续使用 Hive SQL。

- **选择 Spark on Hive**：
  - 如果你需要更高的性能和更低的查询延迟。
  - 如果你需要支持实时分析、流处理或机器学习。
  - 如果你希望使用 Spark 的多种 API（如 DataFrame、RDD）。

---

### 5. **总结**
- **Hive on Spark** 是 Hive 主导的集成方式，适合传统批处理场景。
- **Spark on Hive** 是 Spark 主导的集成方式，适合高性能和实时计算场景。

如果你的业务已经满足于 Hive on Spark 的性能，且没有实时计算需求，可以继续使用 Hive on Spark。如果你需要更高的性能和更灵活的计算模式，建议考虑 Spark on Hive 或独立部署 Spark 集群。



# Hive安装部署以及切换tez引擎

1.上传解压

```bash
# 将Tez解压到创建好的目录中
tar -zxvf apache-tez-0.10.4-bin.tar.gz  -C /data/u01/app/tez
```

2.将tez安装包上传至HDFS

```bash
# 在HDFS上创建目录
hdfs dfs -mkdir -p /user/tez/
# 上传安装包并重命名，这里不需要解压，直接上传即可
hdfs dfs -put apache-tez-0.10.4-bin.tar.gz /user/tez/tez.tar.gz
```

3.修改环境变量

```bash
vim /etc/profile

# 在最后加入以下内容
# TEZ_HOME
export TEZ_HOME=/data/u01/app/tez/apache-tez-0.10.4-bin
export TEZ_CONF_DIR=$HADOOP_CONF_DIR
export TEZ_JARS=$TEZ_HOME/*.jar:$TEZ_HOME/lib/*.jar
export HADOOP_CLASSPATH=$TEZ_CONF_DIR:$TEZ_JARS:$HADOOP_CLASSPATH

source /etc/profile

```

4.增加tez-site.xml配置文件

```bash
# 切换到Hadoop的配置文件所在目录，一般将tez的配置文件也放在此位置
cd /data/u01/app/hadoop/hadoop-3.3.6/etc/hadoop
# 创建并编辑文件
vim tez-site.xml

# 加入以下内容
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
  <!-- 指定在hdfs上的tez包文件 -->
  <property>
    <name>tez.lib.uris</name>
    <value>hdfs://cesdb:8020/user/tez/tez.tar.gz</value>
  </property>
  <!-- tez可使用集群的jar包 -->
  <property>
     <name>tez.use.cluster.hadoop-libs</name>
     <value>true</value>
  </property>
</configuration>
```

5.修改hadoop相关配置文件

Hadoop是分布式模式运行，配置修改完成之后记得分发并重启集群

* yarn-site.xml

```bash
# 主要是yarn.nodemanager.vmem-check-enabled以及yarn.nodemanager.pmem-check-enabled这两项配置
# 由于机器资源有限，故关闭对内存的检查，否则任务运行容易出现资源不够的问题

<property>
    <name>yarn.nodemanager.vmem-check-enabled</name>
    <value>false</value>
</property>
<property>
    <name>yarn.nodemanager.pmem-check-enabled</name>
    <value>false</value>
</property>

```

* mapred-site.xml

```bash
# 主要是修改mapreduce.framework.name配置的值
# 原本应该是yarn，现在改为yarn-tez，其他保持不变

<property>
    <name>mapreduce.framework.name</name>
    <value>yarn-tez</value>
</property>
```

* capacity-scheduler.xml

```bash
# 主要调整一下容量调度中AM的资源占比，保证yarn中能同时运行多个任务
# yarn.scheduler.capacity.maximum-am-resource-percent：AM能够使用的最大资源占比
# 原本为0.1，这里直接改为1，也可根据情况自行调整
# 不调整的话会导致Tez启动等待资源时间过长，进而影响整个HQL的查询时间

<property>
	<name>yarn.scheduler.capacity.maximum-am-resource-percent</name>
	<value>1</value>
	<description>
  		Maximum percent of resources in the cluster which can be used to run
  		application masters i.e. controls number of concurrent running
  		applications.
	</description>
</property>

```

* 分发配置文件

```bash
# 分发给cesdb1
scp -r /data/u01/app/tez root@cesdb1:/data/u01/app
scp -r /data/u01/app/hadoop/hadoop-3.3.6/etc/hadoop hadoop@cesdb1:/data/u01/app/hadoop/hadoop-3.3.6/etc
# 分发给cesdb2
scp -r /data/u01/app/tez root@cesdb2:/data/u01/app
scp -r /data/u01/app/hadoop/hadoop-3.3.6/etc/hadoop hadoop@cesdb2:/data/u01/app/hadoop/hadoop-3.3.6/etc
```

* 重启yarn集群

```bash
# 由于并未涉及到HDFS配置文件，故只需重启Yarn服务即可

stop-yarn.sh
start-yarn.sh
```

6.修改Hive配置文件

```bash
1、主要修改Hive执行引擎为Tez
2、由于资源有限，故还需设置Tez默认执行容器内存大小，否则Tez任务运行容易出现资源问题
3、hive.cli.tez.session.async：是否异步启动Tez会话，默认为true
​ 建议禁止，否则在Tez会话启动前就进入了Hive命令行，容易导致任务首次执行不成功
​ 如若开启，则需进入hive命令行速度会快一些，但成功执行HQL得等待Tez会话创建成功


# 切换目录
cd /data/u01/app/hive/apache-hive-3.1.3/conf
# 编辑hive-site.xml
vim hive-site.xml

# 增加下列配置，其他保持不变
  <property>
    <name>hive.execution.engine</name>
    <value>tez</value>
    <description>
      Expects one of [mr, tez, spark].
      Chooses execution engine. Options are: mr (Map reduce, default), tez, spark. While MR
      remains the default engine for historical reasons, it is itself a historical engine
      and is deprecated in Hive 2 line. It may be removed without further warning.
    </description>
  </property>

  <property>
    <name>hive.tez.container.size</name>
    <value>2048</value>
    <description>By default Tez will spawn containers of the size of a mapper. This can be used to overwrite.</description>
  </property>

  <property>
    <name>hive.cli.tez.session.async</name>
    <value>false</value>
    <description>
      Whether to start Tez
      session in background when running CLI with Tez, allowing CLI to be available earlier.
    </description>
  </property>
  
  
vim hive-env.sh

export TEZ_HOME=/data/u01/app/tez/apache-tez-0.10.4-bin
export TEZ_JARS=""
for jar in `ls $TEZ_HOME |grep jar`; do
export TEZ_JARS=$TEZ_JARS:$TEZ_HOME/$jar
done
for jar in `ls $TEZ_HOME/lib`; do
export TEZ_JARS=$TEZ_JARS:$TEZ_HOME/lib/$jar
done
export TEZ_JARS=${TEZ_JARS:1}
export HIVE_AUX_JARS_PATH=$TEZ_JARS


```

7.解决log4j冲突问题

```bash
# 由于hadoop、hive、tez包中都包含了log4j的依赖，一起搭配使用会造成冲突
# 故只保留hadoop自带的即可，将hive、tez对应的jar包重命名即可
# 切换tez下的lib目录
cd /data/u01/app/tez/apache-tez-0.10.4-bin/lib
# 将log4j对应jar包进行重命名
mv slf4j-log4j12-1.7.25.jar slf4j-log4j12-1.7.25.jar.bak

```

8.控制打印日志级别

```bash
# 切换目录
cd /data/u01/app/hive/apache-hive-3.1.3/conf
# 创建log4j默认配置文件并编辑
vim log4j.properties

# 加入一下内容，通过log4j.rootLogger可控制日志打印级别
log4j.rootLogger=WARN, CA
log4j.appender.CA=org.apache.log4j.ConsoleAppender
log4j.appender.CA.layout=org.apache.log4j.PatternLayout
log4j.appender.CA.layout.ConversionPattern=%-4r [%t] %-5p %c %x - %m%n

# 分发文件
scp -r /data/u01/app/hive/apache-hive-3.1.3/conf hadoop@cesdb1:/data/u01/app/hive/apache-hive-3.1.3/conf
scp -r /data/u01/app/hive/apache-hive-3.1.3/conf hadoop@cesdb2:/data/u01/app/hive/apache-hive-3.1.3/conf
```





# Cron定时调度hive脚本

使用 `cron` 来定时调度 Hive SQL 脚本是一个常见的方式。下面是一个完整的步骤，展示如何编写和配置脚本，以定时运行 Hive SQL 脚本。

1. 编写 Hive SQL 脚本

首先，创建一个 Hive SQL 脚本，例如 `query.hql`：

```sql

CREATE EXTERNAL TABLE IF NOT EXISTS test.dwd_bagl_patientvisit_mi (
  FID BIGINT,
  FPRN STRING,
  FTIMES INT,
  FICDVERSION TINYINT,
  FZYID STRING,
  FAGE STRING,
  FNAME STRING,
  FSEXBH STRING,
  FSEX STRING,
  FBIRTHDAY TIMESTAMP,
  fcydate TIMESTAMP
)
PARTITIONED BY (year STRING, month STRING)
STORED AS ORC
LOCATION '/user/hive/warehouse/test.db/test_hivesql';

SET hive.exec.dynamic.partition=true;
SET hive.exec.dynamic.partition.mode=nonstrict;
INSERT OVERWRITE TABLE test.dwd_bagl_patientvisit_mi PARTITION(year, month)
SELECT 
  FID,
  FPRN,
  FTIMES,
  FICDVERSION,
  FZYID,
  FAGE,
  FNAME,
  FSEXBH,
  FSEX,
  FBIRTHDAY,
  fcydate,
  year(fcydate) AS year,
  month(fcydate) AS month
FROM test.ods_batj_patientvisit_mi
WHERE fcydate >= '2024-01-01' AND fcydate <= '2024-05-31';
```

将该脚本保存到某个目录，例如 `/path/to/query.hql`。

2. 编写 Shell 脚本

创建一个 Shell 脚本，例如 `run_hive_query.sh`，用来运行 Hive SQL 脚本：

```bash
#!/bin/bash

# 定义 Hive 脚本路径
HQL_SCRIPT="/data/chenli/hive/dwd_bagl_patientvisit_mi_test.hql"
LOG_DIR="/data/chenli/cron/logs"
LOG_FILE="${LOG_DIR}/hive_query_$(date +\%Y\%m\%d\%H\%M\%S).log"

# 运行 Hive 脚本
beeline -u 'jdbc:hive2://localhost:10000' -n hadoop -p hadoop -f "$HQL_SCRIPT" > "$LOG_FILE" 2>&1

# 检查执行结果
if [ $? -ne 0 ]; then
  echo "Hive query failed" | mail -s "Hive Query Failed" ****@qq.com
else
  echo "Hive query succeeded" | mail -s "Hive Query Succeeded" ****@qq.com
fi
```

将该脚本保存到某个目录，例如 `/path/to/run_hive_query.sh`。

如果有多个hive任务，可以按照以下方式

```shell
#!/bin/bash

# 定义 Hive 脚本路径
HQL_SCRIPT1="/path/to/query1.hql"
HQL_SCRIPT2="/path/to/query2.hql"

# 定义日志路径
LOG_DIR="/path/to/logs"

# 运行第一个 Hive 脚本
beeline -u 'jdbc:hive2://your-hive-server:10000/default' -n your_username -p your_password -f "$HQL_SCRIPT1" > "$LOG_DIR/hive_query1_$(date +\%Y\%m\%d\%H\%M\%S).log" 2>&1

# 检查第一个脚本的执行结果
if [ $? -ne 0 ]; then
  echo "Hive query1 failed" | mail -s "Hive Query1 Failed" your_email@example.com
  exit 1
fi

# 运行第二个 Hive 脚本
beeline -u 'jdbc:hive2://your-hive-server:10000/default' -n your_username -p your_password -f "$HQL_SCRIPT2" > "$LOG_DIR/hive_query2_$(date +\%Y\%m\%d\%H\%M\%S).log" 2>&1

# 检查第二个脚本的执行结果
if [ $? -ne 0 ]; then
  echo "Hive query2 failed" | mail -s "Hive Query2 Failed" your_email@example.com
  exit 1
fi

```



3. 确保 Shell 脚本具有执行权限

给 Shell 脚本添加执行权限：

```bash
chmod +x /path/to/run_hive_query.sh
```

4. 配置 Crontab 定时任务

编辑 Crontab 文件以配置定时任务：

```bash
crontab -e
```

添加以下行以每天定时运行 Hive SQL 脚本。例如，配置每天凌晨 1 点运行脚本：

```bash
0 1 * * * /path/to/run_hive_query.sh
```

5. 检查和验证 Crontab 配置

保存并退出编辑器后，验证 Crontab 配置是否生效：

```bash
crontab -l
```

6. 监控和日志管理

确保 Shell 脚本将日志输出重定向到指定的日志文件夹，以便后续检查：

```bash
beeline -u 'jdbc:hive2://your-hive-server:10000/default' -n your_username -p your_password -f "$HQL_SCRIPT" > /path/to/logs/hive_query_$(date +\%Y\%m\%d\%H\%M\%S).log 2>&1
```

7. 脚本错误通知

为了确保在脚本执行失败时能收到通知，使用邮件发送错误信息：

```bash
if [ $? -ne 0 ]; then
  echo "Hive query failed" | mail -s "Hive Query Failed" your_email@example.com
fi
```



# Zookeeper集群搭建

Zookeeper集群由基数台机器组成，分为leader和follower两个角色。写入数据时，要写入leader，leader同意后，再通知follower写入。客户端读取数据时，由于数据都是一样的，可以从任意一台机器上读取。而leader是选举出来的，集群中任意一台机器发现没有leader时，则会推荐自己为leader，当超过半数的机器同意它为leader时，选举结束。这样当leader宕机后很快就会选举出新的leader，保证工作正常进行。

## 前期准备

官网下载：[zookeeper下载](https://downloads.apache.org/zookeeper/)，这里我下载的是3.8.4的版本，这个版本相对稳定。要下载apache-zookeeper-3.8.4-bin.tar.gz这个版本的包，带有bin的包，不然启动会报错的。

上传服务器并解压

```
tar -zxvf  apache-zookeeper-3.8.4-bin.tar.gz -C /data/u01/app/zookeeper/
```

配置环境变量

```bash
vi /etc/profile

export ZOOKEEPER_HOME=/data/u01/app/zookeeper/zookeeper-3.8.4
export PATH=$PATH:$ZOOKEEPER_HOME/bin

source /etc/profile
```

## 修改配置文件

配置 zoo.cfg文件

```bash
cd conf
cp zoo_sample.cfg zoo.cfg

# dataDir属性设置Zookeeper的数据存放的位置 /data/u01/app/zookeeper/zookeeper-3.8.4/data/zData
vi zoo.cfg

# The number of milliseconds of each tick
tickTime=2000
# The number of ticks that the initial 
# synchronization phase can take
initLimit=10
# The number of ticks that can pass between 
# sending a request and getting an acknowledgement
syncLimit=5
# the directory where the snapshot is stored.
# do not use /tmp for storage, /tmp here is just 
# example sakes.
dataDir=/data/u01/app/zookeeper/zookeeper-3.8.4/data/zData
# the port at which the clients will connect
clientPort=2181

#  参数说明：
tickTime=2000 表示zookeeper服务器之间或客户端与服务器之间维持心跳的时间间隔，时间单位是毫秒。
initLimit=10 表示Zookeeper服务器集群中连接到leader的Follower服务器初始化连接是最长能忍受多少个心跳的时间间隔数，总的时间长度就是10*2000=20秒。
syncLimit=5 表示标识leader与follower之间发送消息，请求和应答时间长度。最长不能超过多少个tickTime的时间长度，总的时间长度就是5*2000=10秒。
dataDir=/tmp/zookeeper 表示zookeeper的数据存放目录，默认在/tmp/zookeeper，此为修改项，默认情况下Zookeeper将写数据的日志文件也保存在这个目录里，也可以手动指定日志存放的目录。
clientPort=2181 表示客户端（应用程序）连接Zookeeper服务器的端口，Zookeeper会监听这个端口接受客户端的访问请求，默认为2181。
```

指定各个节点信息

```bash
# 打开zoo.cfg文件，在末尾添加几行
server.1=cesdb:2888:3888
server.2=cesdb1:2888:3888
server.3=cesdb2:2888:3888

# 参数说明
各个节点的信息格式为： server.X=A:B:C
X-->指zookeeper节点的myid，标识这个是第几号服务器。
A-->指每个节点的IP地址或者主机名。
B-->指follower和leader交换消息所使用的端口。
C-->指选举leader所使用的端口。

```

创建myid文件

```bash
# 进入zData目录
cd /data/u01/app/zookeeper/zookeeper-3.8.4/data/zData/

vim myid
1
# 输入1即可，另外两台节点分别为2、3
```

## 分发节点

```bash
scp -r /data/u01/app/zookeeper/ root@cesdb1:/data/u01/app
scp -r /data/u01/app/zookeeper/ root@cesdb2:/data/u01/app

# 配置另外两台机器的myid，分别配置2、3
```

## 启动Zookeeper集群及查看状态

```bash
# 需要在每一台机器上执行此命令
zkServer.sh start 

# 查看集群状态
zkServer.sh status

# 关闭zookeeper命令
zkServer.sh stop

# 查看jps
[root@cesdb bin]# jps.sh
==============查询当前所有服务器的jps情况==============
**************cesdb当前jps情况***************
10113 DataNode
10787 ResourceManager
149575 Jps
9893 NameNode
11014 NodeManager
145358 QuorumPeerMain(zookeeper线程)
13933 RunJar
13934 RunJar
**************cesdb1当前jps情况***************
155888 DataNode
109528 QuorumPeerMain(zookeeper线程)
156072 SecondaryNameNode
156361 NodeManager
112141 Jps
**************cesdb2当前jps情况***************
131122 Jps
136547 NodeManager
136208 DataNode
129818 QuorumPeerMain(zookeeper线程)
=======================查询完毕========================
```

## 问题

### 启动失败

[参考链接](https://blog.csdn.net/succing/article/details/127837281)

```
[root@cesdb bin]# zkServer.sh start
ZooKeeper JMX enabled by default
Using config: /data/u01/app/zookeeper/zookeeper-3.8.4/bin/../conf/zoo.cfg
Starting zookeeper ... FAILED TO START

# 查看日志有以下错误
[root@cesdb logs]# cat zookeeper-root-server-cesdb.out 
错误: 找不到或无法加载主类 org.apache.zookeeper.server.quorum.QuorumPeerMain

```

那是因为我是按照3.4的zookeeper去安装的，原因是下载的zookeeper-3.8.4.tar.gz是未编译的，自zk3.5.5版本以后，已编译的jar包，尾部有bin，应该使用的是zookeeper-3.8.4-bin.tar.gz



# DolphinScheduler分布式搭建

## 参考资料

[官方文档](https://dolphinscheduler.apache.org/zh-cn/docs/3.2.2/guide/installation/cluster)

[集群部署方案（2 Master + 3 Worker）](https://www.cnblogs.com/DolphinScheduler/p/18066148)

## 集群部署方案（2Master + 3Worker）

- [Apache DolphinScheduler官网](https://dolphinscheduler.apache.org/zh-cn)

- [Apache DolphinScheduler使用文档](https://dolphinscheduler.apache.org/zh-cn/docs/3.2.2)

- 截止2024-08-26，最新版本：3.2.2

- 部署版本：apache-dolphinscheduler-3.2.2-bin.tar.gz

  | 主机名 | ip            | 部署服务                              | 系统配置          |
  | ------ | ------------- | ------------------------------------- | ----------------- |
  | cesdb  | 10.201.100.75 | MasterServer、WorkerServer、ApiServer | cpu16核 + 内存64G |
  | cesdb1 | 10.201.100.84 | MasterServer、WorkerServer            | cpu16核 + 内存32G |
  | cesdb2 | 10.201.100.85 | WorkerServer、AlertServer             | cpu16核 + 内存32G |

## 前置准备工作

- 操作系统：Red Hat Enterprise 7.9
- JDK：1.8+
- 数据库：Mysql(8.0.38)，驱动8.0.33
- zookeeper：3.8.4
- 注意：Apache DolphinScheduler 本身不依赖 Hadoop、Hive、Spark，但如果你运行的任务需要依赖他们，就需要有对应的环境支持。

**端口说明**

| 组件                 | 默认端口 | 说明                               |
| -------------------- | -------- | ---------------------------------- |
| MasterServer         | 5678     | 非通信端口，只需本机端口不冲突即可 |
| WorkerServer         | 1234     | 非通信端口，只需本机端口不冲突即可 |
| ApiApplicationServer | 12345    | 提供后端通信端口                   |

## 集群部署

**时间同步**

参考上面Hadoop的时间同步

**配置用户、权限(三台机器都要配置)**

```bash
# 创建用户需使用 root 登录
useradd dolphinscheduler

# 添加密码
echo "dolphinscheduler_0753" | passwd --stdin dolphinscheduler

# 配置 sudo 免密
sed -i '$adolphinscheduler  ALL=(ALL)  NOPASSWD: NOPASSWD: ALL' /etc/sudoers
sed -i 's/Defaults    requirett/#Defaults    requirett/g' /etc/sudoers
```

**配置集群免密登录(三台机器都要配置)**

```bash
# 使用创建的 dolphinscheduler 登陆，配置hadoop31到hadoop32、hadoop33免密登陆
su dolphinscheduler

# hadoop31节点，生成密钥
ssh-keygen -t rsa

# hadoop31节点操作，配置向hadoop31、hadoop32、hadoop33节点免密
ssh-copy-id cesdb
ssh-copy-id cesdb1
ssh-copy-id cesdb2

```

**zookeeper集群启动**

```bash
zk-all.sh start
```

## 初始化数据库

**创建数据库、用户、授权**

```bash
-- 进入MySQL命令行
[root@hadoop01]# mysql -u root -p 
Enter password: xxxxxx
-- 创建 dolphinscheduler 数据库用户和密码，并限定登陆范围
mysql > CREATE USER 'dolphinscheduler'@'%' IDENTIFIED BY '******';
-- 创建 dolphinscheduler 的元数据，并指定编码
mysql > CREATE DATABASE dolphinscheduler DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;
-- 为dolphinscheduler数据库授权
mysql > grant all privileges on dolphinscheduler.* to 'dolphinscheduler'@'%';
-- 刷新权限
mysql > flush privileges;
```

**解压安装包**

```bash
su root
# 解压
tar zxvf apache-dolphinscheduler-3.2.2-bin.tar.gz -C /data/u01/app/dolphinscheduler

# 修改目录权限，使得部署用户对解压缩后的文件有操作权限（三个节点都要这样操作，不然会报权限不足的错误，导致启动不了）
chown -R dolphinscheduler:dolphinscheduler dolphinscheduler
chmod -R 755 dolphinscheduler
```

**添加mysql驱动至lib目录**

将驱动移动至 DolphinScheduler 的每个模块下的 libs 目录下。共5个目录：

- api-server/libs

- alert-server/libs

- master-server/libs

- worker-server/libs

- tools/libs

  ```bash
  cp mysql-connector-j-8.0.33.jar /data/u01/app/dolphinscheduler/apache-dolphinscheduler-3.2.2-bin/api-server/libs
  
  cp mysql-connector-j-8.0.33.jar /data/u01/app/dolphinscheduler/apache-dolphinscheduler-3.2.2-bin/alert-server/libs
  
  cp mysql-connector-j-8.0.33.jar /data/u01/app/dolphinscheduler/apache-dolphinscheduler-3.2.2-bin/master-server/libs
  
  cp mysql-connector-j-8.0.33.jar /data/u01/app/dolphinscheduler/apache-dolphinscheduler-3.2.2-bin/worker-server/libs
  
  cp mysql-connector-j-8.0.33.jar /data/u01/app/dolphinscheduler/apache-dolphinscheduler-3.2.2-bin/tools/libs
  
  ```

## 修改配置文件

**dolphinscheduler_env.sh**

```bash
cd /data/u01/app/dolphinscheduler/apache-dolphinscheduler-3.2.2-bin/bin/env

# 修改dolphinscheduler_env.sh
vim dolphinscheduler_env.sh

# 在文末添加以下配置：
# JAVA_HOME, will use it to start DolphinScheduler server
# JDK配置
export JAVA_HOME=${JAVA_HOME:-/usr/jdk}

# Database related configuration, set database type, username and password
# MySQL数据库配置
export DATABASE=${DATABASE:-mysql}
export SPRING_PROFILES_ACTIVE=${DATABASE}
export SPRING_DATASOURCE_URL="jdbc:mysql://10.201.100.75:3306/dolphinscheduler?useUnicode=true&characterEncoding=UTF-8&useSSL=false"
export SPRING_DATASOURCE_USERNAME=${SPRING_DATASOURCE_USERNAME:-"dolphinscheduler"}
export SPRING_DATASOURCE_PASSWORD=${SPRING_DATASOURCE_PASSWORD:-"dolphinscheduler_0753"}

# DolphinScheduler server related configuration
export SPRING_CACHE_TYPE=${SPRING_CACHE_TYPE:-none}
export SPRING_JACKSON_TIME_ZONE=${SPRING_JACKSON_TIME_ZONE:-UTC}
export MASTER_FETCH_COMMAND_NUM=${MASTER_FETCH_COMMAND_NUM:-10}

# Registry center configuration, determines the type and link of the registry center
# zk注册中心
export REGISTRY_TYPE=${REGISTRY_TYPE:-zookeeper}
export REGISTRY_ZOOKEEPER_CONNECT_STRING=${REGISTRY_ZOOKEEPER_CONNECT_STRING:-cesdb:2181,cesdb1:2181,cesdb2:2181}

# Tasks related configurations, need to change the configuration if you use the related tasks.
# 其他环境配置(此处只配置了hadoop、hive，其他环境未部署)
# 如果你不使用某些任务类型，可以忽略不做配置，使用默认即可。比如Flink不使用，不做处理即可
export HADOOP_HOME=${HADOOP_HOME:-/data/u01/app/hadoop/hadoop-3.3.6}
export HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-/data/u01/app/hadoop/hadoop-3.3.6/etc/hadoop}
export HIVE_HOME=${HIVE_HOME:-/data/u01/app/hive/apache-hive-3.1.3}
# export SPARK_HOME=${SPARK_HOME:-/opt/soft/spark}
# export PYTHON_LAUNCHER=${PYTHON_LAUNCHER:-/opt/soft/python}
# export FLINK_HOME=${FLINK_HOME:-/opt/soft/flink}
# export DATAX_LAUNCHER=${DATAX_LAUNCHER:-/opt/soft/datax/bin/python3}

export PATH=$HADOOP_HOME/bin:$JAVA_HOME/bin:$HIVE_HOME/bin:$PATH
```

**编辑install_env.sh文件**

```bash
# 修改install_env.sh
vim /data/u01/app/dolphinscheduler/apache-dolphinscheduler-3.2.2-bin/bin/env/install_env.sh

# 按照集群部署方案，内容如下：

# 集群节点
ips=${ips:-"cesdb,cesdb1,cesdb2"}

# ssh免密端口,使用默认
sshPort=${sshPort:-"22"}

# master节点
masters=${masters:-"cesdb,cesdb1"}

# worker节点
workers=${workers:-"cesdb:default,cesdb1:default,cesdb2:default"}

# alert节点
alertServer=${alertServer:-"cesdb2"}

# api节点
apiServers=${apiServers:-"cesdb"}

# dolphinscheduler实际安装路径
installPath=${installPath:-"/data/u01/app/dolphinscheduler"}

# 部署dolphinscheduler使用的用户名
deployUser=${deployUser:-"dolphinscheduler"}

# zk根节点
zkRoot=${zkRoot:-"/dolphinscheduler"}

```

**初始化元数据**

```bash
# 切换到apache-dolphinscheduler-3.2.2-bin目录下，执行命令
[dolphinscheduler@cesdb apache-dolphinscheduler-3.2.2-bin]$ sh ./tools/bin/upgrade-schema.sh

# 此操作，会向MySQL数据库写入元数据
```

## 分发文件

```bash
scp -r /data/u01/app/dolphinscheduler root@cesdb1:/data/u01/app/
scp -r /data/u01/app/dolphinscheduler root@cesdb2:/data/u01/app/

chown -R dolphinscheduler:dolphinscheduler dolphinscheduler
```

## 启动DolphinScheduler

```bash
# 一键停止集群所有服务
bash ./bin/stop-all.sh

# 一键开启集群所有服务
bash ./bin/start-all.sh

# 启停 Master(cesdb、cesdb1分别启动)
bash ./bin/dolphinscheduler-daemon.sh start master-server
bash ./bin/dolphinscheduler-daemon.sh stop master-server

# 启停 Worker(cesdb、cesdb1、cesdb2分别启动)
bash ./bin/dolphinscheduler-daemon.sh start worker-server
bash ./bin/dolphinscheduler-daemon.sh stop worker-server

# 启停 Api(cesdb启动)
bash ./bin/dolphinscheduler-daemon.sh start api-server
bash ./bin/dolphinscheduler-daemon.sh stop api-server

# 启停 Alert(cesdb2启动)
bash ./bin/dolphinscheduler-daemon.sh start alert-server
bash ./bin/dolphinscheduler-daemon.sh stop alert-server
```

**查看zookeeper节点信息**

```bash
[zk: localhost:2181(CONNECTED) 5] ls /dolphinscheduler/nodes/master 
[10.201.100.75:5678, 10.201.100.84:5678]
[zk: localhost:2181(CONNECTED) 6] ls /dolphinscheduler/nodes 
[alert-server, master, worker]
[zk: localhost:2181(CONNECTED) 7] ls /dolphinscheduler/nodes/worker 
[10.201.100.75:1234, 10.201.100.84:1234, 10.201.100.85:1234]
[zk: localhost:2181(CONNECTED) 8] ls /dolphinscheduler/nodes/alert-server 
[10.201.100.85:50052]

```



## 访问

```bash
http://10.201.100.75:12345/dolphinscheduler/ui
```

![界面图](D:\Github\MyKnowledgeRepository\img\bigdata\dolphinscheduler\安装成功界面图.png)



# Ranger搭建

Ranger2.0要求对应的Hadoop为3.x以上，Hive为3.x以上版本，JDK为1.8以上版本！

## 安装maven

**参考资料：**

[Linux安装maven(详细教程)](https://www.cnblogs.com/fuzongle/p/12825048.html)

[maven私服搭建详细教程](https://blog.csdn.net/qq_50652600/article/details/131066029)

1.解压maven

```bash
tar -zxvf apache-maven-3.8.8-bin.tar.gz -C /data/u01/app/maven
```

2.配置maven仓库，设置阿里镜像仓库，一定要配置一下，国内的下载jar快些，首先进入cd apache-maven-3.6.3目录，创建仓库存储目录，mkdir ck。

```bash
cd apache-maven-3.8.8/
 
mkdir ck
 
cd conf
 
vim settings.xml

# 添加本地存储的位置
<localRepository>/data/u01/app/maven/apache-maven-3.8.8/ck</localRepository>

# 添加镜像
<mirror>
      <id>maven-nexus</id>
      <name>nexus maven</name>
       <url>http://172.16.200.77:8081/repository/maven-public/</url>
      <mirrorOf>*</mirrorOf>
</mirror>

# 因为nexus需要有用户名和密码才能访问，所以需要在setting.xml文件中配置账号密码
<server>
  <id>maven-nexus</id>
  <username>admin</username>
  <password>Mzrmyy@0753</password>
</server>

# 希望所有的 Maven 项目都使用内部的私服仓库来解析依赖,明确禁用对外网仓库的访问，以防止 Maven 在找不到依赖项时尝试连接外网
<profiles>
    <profile>
        <id>internal-repo</id>
        <repositories>
            <repository>
                <id>central</id>
                <url>http://172.16.200.77:8081/repository/maven-public/</url>
                <releases><enabled>true</enabled></releases>
                <snapshots><enabled>true</enabled></snapshots>
            </repository>
        </repositories>
    </profile>
</profiles>

<activeProfiles>
    <activeProfile>internal-repo</activeProfile>
</activeProfiles>

```

3.配置环境变量

```bash
vim /etc/profile

export MAVEN_HOME=/data/u01/app/maven/apache-maven-3.8.8
export PATH=$PATH:$MAVEN_HOME/bin

source /etc/profile
```

4.测试

```bash
mvn -v

[root@cesdb apache-maven-3.8.8]# mvn -v
Apache Maven 3.8.8 (4c87b05d9aedce574290d1acc98575ed5eb6cd39)
Maven home: /data/u01/app/maven/apache-maven-3.8.8
Java version: 1.8.0_333, vendor: Oracle Corporation, runtime: /usr/jdk/jre
Default locale: zh_CN, platform encoding: UTF-8
OS name: "linux", version: "3.10.0-1160.el7.x86_64", arch: "amd64", family: "unix"
```

报错处理：

如果maven编译过程中提示没有相应的包，可以下载相应的maven包上传到私服上面

![0e33d2a736be67cba0ae14db82974ff](D:\文档\WeChat Files\wxid_dd5hc1oetwwy21\FileStorage\Temp\0e33d2a736be67cba0ae14db82974ff.png)

![ef580d18708a1a372c5c585fc267110](D:\文档\WeChat Files\wxid_dd5hc1oetwwy21\FileStorage\Temp\ef580d18708a1a372c5c585fc267110.png)



## 源码编译

```bash
tar -zxvf apache-ranger-2.5.0.tar.gz -C /data/u01/app/ranger/

cd /data/u01/app/ranger/apache-ranger-2.5.0

mvn clean compile package install -DskipTests

```

如果编译失败的就直接使用现成的吧，别人编译好的源码。

## 安装RangerAdmin

1.数据库环境准备

```mysql
# 创建数据库
create database ranger;

# 创建用户
CREATE USER 'ranger'@'%' IDENTIFIED BY '******';

# 授予权限
GRANT ALL PRIVILEGES ON ranger.* TO 'ranger'@'%';

FLUSH PRIVILEGES;
```

2.解压

```bash
[root@cesdb ranger-package]# tar -zxvf ranger-2.4.1-SNAPSHOT-admin.tar.gz -C /data/u01/app/ranger/ranger-2.4.1/
[root@cesdb ranger-2.4.1]# mv ranger-2.4.1-SNAPSHOT-admin/ ranger-admin/
```

3.修改配置

```bash
[root@cesdb ranger-admin]# vim install.properties 

SQL_CONNECTOR_JAR=/data/u01/soft/database_connector_jar/mysql-connector-j-8.0.33.jar
# mysql主机和root用户的用户密码
db_root_user=root
db_root_password=******
db_host=cesdb
# ranger需要的数据库名和用户信息
db_name=ranger
db_user=ranger
db_password=******
# 其他ranger admin需要的用户密码
rangerAdmin_password=******
rangerTagsync_password=******
rangerUsersync_password=******
keyadmin_password=******
#ranger存储审计日志的路径，默认为solr，这里为了方便暂不设置
audit_store=
# 策略管理器的url,rangeradmin安装在哪台机器，主机名就为对应的主机名
policymgr_external_url=http://cesdb:6080
# 启动ranger admin进程的linux用户信息
unix_user=hadoop
unix_user_pwd=hadoop
unix_group=hadoop
# hadoop的配置文件目录
hadoop_conf=/data/u01/app/hadoop/hadoop-3.3.6/etc/hadoop

```

4.执行安装

```bash
[root@cesdb ranger-admin]# ./setup.sh 
```

5.创建软连接

```bash
[root@cesdb ranger-admin]# ./set_globals.sh 
usermod：无改变
[2024/10/11 15:15:29]:  [I] Soft linking /etc/ranger/admin/conf to ews/webapp/WEB-INF/classes/conf
```

6.修改ranger-admin-site.xml

```bash
 <property>
        <name>ranger.jpa.jdbc.user</name>
        <value>ranger</value>
        <description />
</property>
<property>
        <name>ranger.jpa.jdbc.password</name>
        <value>******</value>
        <description />
</property>
```

7.启动

```bash
[root@cesdb ranger-admin]# ranger-admin start
Starting Apache Ranger Admin Service
Apache Ranger Admin Service with pid 69232 has started.
[root@cesdb ranger-admin]# jps
16096 MasterServer
86595 RunNiFi
99462 DataNode
16168 WorkerServer
100488 NodeManager
69232 EmbeddedServer(ranger-admin进程)
121875 RunJar
83634 QuorumPeerMain
121876 RunJar
86647 NiFi
99193 NameNode
16254 ApiApplicationServer
100255 ResourceManager

# 停止
ranger-admin stop
```





## 问题总结

1.安装ranger-admin时报错，mysql主机阻塞，日志如下：

```
2024-10-11 08:05:09,937  [JISQL] /usr/jdk/bin/java  -cp /data/u01/soft/database_connector_jar/mysql-connector-j-8.0.33.jar:/data/u01/app/ranger/ranger-2.4.1/ranger-admin/jisql/lib/* org.apache.util.sql.Jisql -driver mysqlconj -cstring jdbc:mysql://cesdb/mysql -u root -p '********' -noheader -trim -c \; -query "SELECT version();"
Loading class `com.mysql.jdbc.Driver'. This is deprecated. The new driver class is `com.mysql.cj.jdbc.Driver'. The driver is automatically registered via the SPI and manual loading of the driver class is generally unnecessary.
SQLException : SQL state: HY000 java.sql.SQLException: null,  message from server: "Host '10.201.100.75' is blocked because of many connection errors; unblock with 'mysqladmin flush-hosts'" ErrorCode: 1129
2024-10-11 08:05:10,277  [E] Can't establish db connection.. Exiting..
2024-10-11 08:05:10,285  [E] DB schema setup failed! Please contact Administrator.
```

解决办法：解锁被阻塞的主机

```bash
# 解锁被阻塞的主机
mysql> flush hosts;
Query OK, 0 rows affected, 1 warning (0.00 sec)

# 加最大允许连接错误数，从而减少主机被阻塞的情况
mysql> SET GLOBAL max_connect_errors=1000;
Query OK, 0 rows affected (0.03 sec)

```

2.SQLException : SQL state: 08001 java.sql.SQLNonTransientConnectionException: Public Key Retrieval is not allowed ErrorCode: 0

```bash
从错误日志 Public Key Retrieval is not allowed 可以看出，这次的问题是与 MySQL 8.0 默认的连接安全设置有关。具体是由于 MySQL 8.0 引入了新的身份验证方法，并且默认情况下，不允许通过 JDBC 连接进行公钥检索。此错误通常出现在 MySQL 用户密码使用 caching_sha2_password 认证插件时，或者客户端连接未配置 allowPublicKeyRetrieval=true 参数时。

jdbc:mysql://cesdb/ranger?allowPublicKeyRetrieval=true&useSSL=false

如果修改 JDBC URL 无法解决问题，可以尝试将 Ranger 用户的身份认证方式改为 mysql_native_password。
ALTER USER 'ranger'@'%' IDENTIFIED WITH mysql_native_password BY 'your_password';

SELECT user, host, plugin FROM mysql.user WHERE user = 'ranger';
ALTER USER 'ranger'@'%' IDENTIFIED WITH mysql_native_password BY 'your_password';

```

发现修改了配置执行的时候还是没有allowPublicKeyRetrieval=true这个参数

```bash
/usr/jdk/bin/java -cp /data/u01/soft/database_connector_jar/mysql-connector-java-8.0.26.jar:/data/u01/app/ranger/ranger-2.4.1/ranger-admin/jisql/lib/* org.apache.util.sql.Jisql -driver mysqlconj -cstring "jdbc:mysql://10.201.100.75/ranger?useSSL=false&allowPublicKeyRetrieval=true" -u 'ranger' -p '********' -noheader -trim -c \; -query "select 1;"
```

此时我只能通过修改源码的方式强行添加allowPublicKeyRetrieval=true这个参数

```bash
vim  db_setup.py 

# 找到get_jisql_cmd这个函数。

        def get_jisql_cmd(self, user, password ,db_name):
                path = RANGER_ADMIN_HOME
                db_ssl_param=''
                db_ssl_cert_param=''
                if self.db_ssl_enabled == 'true':
                        db_ssl_param="?useSSL=%s&requireSSL=%s&verifyServerCertificate=%s" %(self.db_ssl_enabled,self.db_ssl_required,self.db_ssl_verifyServerCertificate)
                        if self.db_ssl_verifyServerCertificate == 'true':
                                if self.db_ssl_auth_type == '1-way':
                                        db_ssl_cert_param=" -Djavax.net.ssl.trustStore=%s -Djavax.net.ssl.trustStorePassword=%s " %(self.javax_net_ssl_trustStore,self.javax_net_ssl_trustStorePassword)
                                else:
                                        db_ssl_cert_param=" -Djavax.net.ssl.keyStore=%s -Djavax.net.ssl.keyStorePassword=%s -Djavax.net.ssl.trustStore=%s -Djavax.net.ssl.trustStorePassword=%s " %(self.javax_net_ssl_keyStore,self.javax_net_ssl_keyStorePassword,self.javax_net_ssl_trustStore,self.javax_net_ssl_trustStorePassword)
                else:
                        # 修改源码，添加allowPublicKeyRetrieval 参数
                        if "useSSL" not in db_name:
                                db_ssl_param="?useSSL=false&allowPublicKeyRetrieval=true"
                self.JAVA_BIN = self.JAVA_BIN.strip("'")
                if is_unix:
                        if self.is_db_override_jdbc_connection_string == 'true' and self.db_override_jdbc_connection_string is not None and len(self.db_override_jdbc_connection_string) > 0:
                                jisql_cmd = "%s %s -cp %s:%s/jisql/lib/* org.apache.util.sql.Jisql -driver mysqlconj -cstring %s -u '%s' -p '%s' -noheader -trim -c \;" %(self.JAVA_BIN,db_ssl_cert_param,self.SQL_CONNECTOR_JAR,path,self.db_override_jdbc_connection_string,user,password)
                        else:
                                jisql_cmd = "%s %s -cp %s:%s/jisql/lib/* org.apache.util.sql.Jisql -driver mysqlconj -cstring jdbc:mysql://%s/%s%s -u '%s' -p '%s' -noheader -trim -c \;" %(self.JAVA_BIN,db_ssl_cert_param,self.SQL_CONNECTOR_JAR,path,self.host,db_name,db_ssl_param,user,password)
                elif os_name == "WINDOWS":
                        if self.is_db_override_jdbc_connection_string == 'true' and self.db_override_jdbc_connection_string is not None and len(self.db_override_jdbc_connection_string) > 0:
                                jisql_cmd = "%s %s -cp %s;%s\jisql\\lib\\* org.apache.util.sql.Jisql -driver mysqlconj -cstring %s -u \"%s\" -p \"%s\" -noheader -trim" %(self.JAVA_BIN,db_ssl_cert_param,self.SQL_CONNECTOR_JAR, path, self.db_override_jdbc_connection_string,user, password)
                        else:
                                jisql_cmd = "%s %s -cp %s;%s\jisql\\lib\\* org.apache.util.sql.Jisql -driver mysqlconj -cstring jdbc:mysql://%s/%s%s -u \"%s\" -p \"%s\" -noheader -trim" %(self.JAVA_BIN,db_ssl_cert_param,self.SQL_CONNECTOR_JAR, path, self.host, db_name, db_ssl_param,user, password)
                return jisql_cmd

```

修改完源码之后就可以正常安装启动了。



# superset安装部署（外网环境）

参考资料：[superset安装和部署](https://blog.csdn.net/m0_74756715/article/details/134510760)

[superset官网](https://superset.apache.ac.cn/docs/configuration/databases/)

## 安装conda

下载使用wget

```bash
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
```

安装

```bash
bash Miniconda3-latest-Linux-x86_64.sh
```

配置环境变量

```bash
#配置系统环境环境量
vim /etc/profile
export CONDA_HOME=/data/app/conda
export PATH=$PATH:$CONDA_HOME/bin
#刷新
source /etc/profile
```

加载环境变量使之生效

```bash
source ~/.bashrc
```

取消激活bash环境

```bash
conda config --set auto_activate_base false
```

conda常用命令

```bash
# 查看conda环境
conda env list
# 删除指定环境
conda env remove -n superset
```

配置conda私服源

```bash
vim ~/.condarc

channels:
  - http://172.16.200.77:8081/repository/pypi-group1/
  - defaults
show_channel_urls: true

#不想编辑的话就直接命令添加
conda config --add channels http://172.16.200.77:8081/repository/pypi-group1/
conda config --set show_channel_urls yes

# 验证配置
conda config --show

# 清理缓存
conda clean --all

# 禁用ssl验证（如果有需要）
conda config --set ssl_verify no

# 留个案底
channels:
  -http://172.16.200.77:8081/repository/pypi-tsinghua/
  - http://172.16.200.77:8081/repository/pypi-group1/
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/
  - https://repo.anaconda.com/pkgs/main
  - https://repo.anaconda.com/pkgs/r
show_channel_urls: true
auto_activate_base: false


# 临时安装方式
pip install pyhive --index-url=http://172.16.200.77:8081/repository/pypi-tsinghua/simple --trusted-host=172.16.200.77
pip install sasl --index-url=http://172.16.200.77:8081/repository/pypi-tsinghua/simple --trusted-host=172.16.200.77
pip install thrift --index-url=http://172.16.200.77:8081/repository/pypi-tsinghua/simple --trusted-host=172.16.200.77
pip install thrift-sasl --index-url=http://172.16.200.77:8081/repository/pypi-tsinghua/simple --trusted-host=172.16.200.77
pip install pyhs2 --index-url=http://172.16.200.77:8081/repository/pypi-tsinghua/simple --trusted-host=172.16.200.77
pip install pymysql --index-url=http://172.16.200.77:8081/repository/pypi-tsinghua/simple --trusted-host=172.16.200.77
```

## 创建python3.9环境

配置conda国内镜像

```bash
#包含了一些基本的 Python 包、库和工具。
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/
 
#这是 Anaconda 主要的通道，包含了大量的 Python 包和工具。如果你主要使用 Python 包，这是一个必要的通道。
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/
```

查看镜像

```bash
conda config --show channels
```

创建python 3.9环境

```bash
conda create --name superset python=3.9
```

激活superset环境

```bash
conda activate superset
```

退出当前环境

```bash
conda deactivate
```

## superset部署

配置yum

```bash
cd /etc/yum.repos.d 
vim yum.repo 

[base] 
name=base
baseurl=http://mirrors.aliyun.com/centos/7.9.2009/os/x86_64/
enabled=1
gpgcheck=0

yum clean all
yum makecache
```

安装依赖

```bash
sudo yum install -y gcc gcc-c++ libffi-devel python-devel python-pip python-wheel python-setuptools openssl-devel cyrus-sasl-devel openldap-devel
```

更新setuptools和pip

```bash
pip install --upgrade setuptools pip -i https://mirrors.aliyun.com/pypi/simple/
```

安装superset

```bash
pip install apache-superset -i https://mirrors.aliyun.com/pypi/simple/
```

初始化数据库

```bash
export FLASK_APP=superset # 设置flask_app环境变量
export SUPERSET_CONFIG_PATH=~/.superset/superset_config.py
superset db upgrade
```

如果提示这个

```bash
(superset) [root@localhost yum.repos.d]# superset db upgrade
--------------------------------------------------------------------------------
                                    WARNING
--------------------------------------------------------------------------------
A Default SECRET_KEY was detected, please use superset_config.py to override it.
Use a strong complex alphanumeric string and use a tool to help you generate 
a sufficiently random sequence, ex: openssl rand -base64 42
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
Refusing to start due to insecure SECRET_KEY
(superset) [root@localhost yum.repos.d]# 

# 使用以下命令
openssl rand -base64 42
PU13QHF3pA+6ZxACiOmMrfkqUJPa0dO+AvXVYFaIrfcSYwOFTGWBhe0X

vi ~/.superset/superset_config.py
SECRET_KEY = "PU13QHF3pA+6ZxACiOmMrfkqUJPa0dO+AvXVYFaIrfcSYwOFTGWBhe0X"
# 设置默认语言为中文
# BABEL_DEFAULT_LOCALE = 'zh'
LANGUAGES = {
    'en': {'flag': 'us', 'name': 'English'},
    'zh': {'flag': 'cn', 'name': '中文'},
}

# 重新运行命令
superset db upgrade

```

创建管理员用户

```bash
superset fab create-admin

用户名：superset
密码：superset_0753
```

superset初始化

```bash
superset init
```

## 启动superset

安装gunicorn ，是一个 Python Web Server，可以和 java 中的 TomCat 类比

```bash
pip install gunicorn -i https://mirrors.aliyun.com/pypi/simple/
```

启动superset

```bash
gunicorn --workers 5 --timeout 120 --bind superset:8787 "superset.app:create_app()" --daemon
```

**说明：**

- workers：指定进程个数
- timeout：worker 进程超时时间，超时会自动重启
- bind：绑定本机地址，即为 Superset 访问地址，若使用主机名，需要在本地主机中建立虚拟机主机名和虚拟机ip的映射关系
- daemon：后台运行

停止superset

```bash
ps -ef | awk '/superset/ && !/awk/{print $2}' | xargs kill -9
```

## 登录superset

访问http://superset:8787，使用之前创建的管理员账号进行登录

![](D:\Github\MyKnowledgeRepository\img\bigdata\superset\superset登录界面.png)

如果访问不了，检查一下防火墙状态

```bash
firewall-cmd --state

# 开放8787端口
firewall-cmd --permanent --add-port=8787/tcp
firewall-cmd --reload

# 查看是否监听
ss -tuln | grep 8787

```

如果要通过域名(主机名)访问，需要在/etc/hosts文件中添加以下内容

```bash
172.16.200.116 superset
```



## superset启停脚本

```bash
mkdir /data/app/script
vim superset.sh

#!/bin/bash
superset_status(){
        result=`ps -ef | awk '/gunicorn/ && !/awk/{print $2}' | wc -l`
        if [[ $result -eq 0 ]]; then
                return 0
        else
                return 1
        fi
}
superset_start(){
                source ~/.bashrc
                superset_status >/dev/null 2>&1
                if [[ $? -eq 0 ]]; then
                        conda activate superset ; gunicorn --workers 5 --timeout 120 --bind superset:8787 --daemon 'superset.app:create_app()'
                else
                        echo "superset 正在运行"
                fi
}
superset_stop(){
                superset_status >/dev/null 2>&1
                if [[ $? -eq 0 ]]; then
                        echo "superset 未在运行"
                else
                        ps -ef | awk '/gunicorn/ && !/awk/{print $2}' | xargs kill -9
                fi
}
case $1 in
        start )
                echo "启动 Superset"
                superset_start
        ;;
        stop )
                echo "停止 Superset"
                superset_stop
        ;;
        restart )
                echo "重启 Superset"
                superset_stop
                superset_start
        ;;
        status )
                superset_status >/dev/null 2>&1
                if [[ $? -eq 0 ]]; then
                        echo "superset 未在运行"
                else
                        echo "superset 正在运行"
                fi
esac

chmod +x superset.sh
```

启动superset

```bash
./superset.sh start
```

查看superset状态

```bash
./superset.sh status
```

停止superset

```bash
./superset.sh stop
```

重新启动

```bash
./superset.sh restart
```

## 安装相应的连接库

[连接到数据库](https://superset.apache.ac.cn/docs/configuration/databases/)

```bash
# 安装hive连接
conda install pyhive
```

## 配置Mysql为元数据库

备份sqllite数据库

```bash
# 切换目录
cd ~/.superset/
# 复制并备份
cp superset.db /data/backup/superset_backup.db
# 导出sqllite数据
sqlite3 ~/.superset/superset.db .dump > superset_dump.sql
```

创建数据库

```sql
-- 进入MySQL命令行
[root@hadoop01]# mysql -u root -p 
Enter password: xxxxxx
-- 创建 superset 数据库用户和密码，并限定登陆范围
mysql > CREATE USER 'superset'@'%' IDENTIFIED BY '******';
-- 创建 dolphinscheduler 的元数据，并指定编码
mysql > CREATE DATABASE superset DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;
-- 为dolphinscheduler数据库授权
mysql > grant all privileges on superset.* to 'superset'@'%';
-- 刷新权限
mysql > flush privileges;
```

导入数据到mysql

```bash
mysql -u superset -p superset < superset_dump.sql
```

配置数据库

```bash
vim superset_config.py

SQLALCHEMY_DATABASE_URI = 'mysql://superset:superset_0753@10.201.100.75:3306/superset?charset=utf8'

SQLALCHEMY_DATABASE_URI = 'mysql+pymysql://superset:your_password@10.201.100.75:3306/superset'


```

初始化并升级mysql数据库

```
conda activate superset

superset db upgrade
```

superset服务端如何连接mysql数据库

```bash
(superset) [root@superset scrip]# mysql -u superset -p -h 10.201.100.75 -P 3306 superset
```

## 配置redis数据缓存

1.安装Redis环境（查看另一个安装文档）

2.安装redis的python客户端

```bash
pip install redis --index-url=http://172.16.200.77:8081/repository/pypi-tsinghua/simple --trusted-host=172.16.200.77

pip install cachelib redis --index-url=http://172.16.200.77:8081/repository/pypi-tsinghua/simple --trusted-host=172.16.200.77

```

3.配置superset使用redis作为缓存后端

```bash
# 编辑superset的配置文件superset_config.py

vim ~/.superset/superset_config.py

from cachelib.redis import RedisCache
# Redis连接配置
CACHE_CONFIG = {
    'CACHE_TYPE': 'RedisCache',
    'CACHE_DEFAULT_TIMEOUT': 86400,  # 缓存超时时间（秒）
    'CACHE_REDIS_URL': 'redis://localhost:6379/0'  # Redis实例URL
}
# 启用数据集查询缓存，一定要添加这句话，不然缓存启动不成功
DATA_CACHE_CONFIG = CACHE_CONFIG
```

4.重启superset

5.验证redis缓存是否生效

```bash
# 运行sql查询或查看仪表板，确保数据加载成功
# 检查redis缓存
cd /usr/local/bin
redis-cli
keys *
# 访问同一个仪表板，确保加载速度显著提升


(superset) [root@superset bin]# redis-cli
127.0.0.1:6379> keys *
1) "superset_cache_erNXxT6Ard5FOdvOzlg6Ii"
2) "superset_cache_superset.views.base.cached_common_bootstrap_data_memver"
127.0.0.1:6379> 


redis-cli
SELECT 0  # 对应 CACHE_CONFIG 的数据库
KEYS *
SELECT 1  # 对应 RESULTS_BACKEND 的数据库
KEYS *
ttl key # 查看key过期时间
info stats # 查看缓存命中情况 关注这两个指标 keyspace_hits: 2676 keyspace_misses: 2731
# 目前来看缓存是已经生效了，因为redis是有数据的，但是不知道为什么看板速度还是很慢。
```

## 配置Celery（用于异步查询，可选）

如果需要启用 SQL Lab 的异步查询功能，需要配置 Celery，并使用 Redis 作为消息代理。

1.安装celery

```bash
pip install celery --index-url=http://172.16.200.77:8081/repository/pypi-tsinghua/simple --trusted-host=172.16.200.77
```

2.在superset_config.py中添加celery配置

```bash
# 配置查询结果缓存（可选）
RESULTS_BACKEND = RedisCache(
    host='localhost',
    port=6379,
    db=1,
    default_timeout=86400,
    key_prefix='superset_results_'
)
# 配置celery异步查询
class CeleryConfig(object):
    broker_url = 'redis://localhost:6379/1'  # Redis 作为消息代理
    result_backend = 'redis://localhost:6379/1'  # 任务结果存储
    imports = ('superset.sql_lab',)
    task_annotations = {'tasks.add': {'rate_limit': '10/s'}}
# 这句话也一定要添加
CELERY_CONFIG = CeleryConfig


```

3.启动celery worker

```bash
celery -A superset.tasks.celery_app:app worker --loglevel=info

# 后台启动
nohup celery -A superset.tasks.celery_app:app worker --loglevel=info > celery.log 2>&1 &
ps aux | grep 'celery'

celery -A superset.tasks.celery_app:app status


pip install flower --index-url=http://172.16.200.77:8081/repository/pypi-tsinghua/simple --trusted-host=172.16.200.77

```

4.重启superset

## 配置日志记录

```bash
import os
from logging.config import dictConfig

# 设置日志文件路径
LOG_FILE = "/data/app/superset/logs/superset.log"

# 创建日志目录
os.makedirs(os.path.dirname(LOG_FILE), exist_ok=True)

# 配置日志
LOGGING_CONFIG = {
    "version": 1,
    "disable_existing_loggers": False,
    "formatters": {
        "standard": {
            "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        },
    },
    "handlers": {
        "file": {
            "level": "DEBUG",
            "class": "logging.handlers.TimedRotatingFileHandler",
            "formatter": "standard",
            "filename": LOG_FILE,
            "when": "midnight",
            "backupCount": 7,
        },
    },
    "root": {
        "level": "DEBUG",
        "handlers": ["file"],
    },
}

# 应用日志配置
dictConfig(LOGGING_CONFIG)

```

##  superset可以直接数据库写入用户

```sql
use superset;
desc ab_role;
desc ab_user;
desc ab_user_role;

-- 插入用户
INSERT INTO ab_user (first_name, last_name, username, email, active, created_on, changed_on)
VALUES ('Test', 'User', 'test', 'test@example.com', 1, NOW(), NOW());

-- 获取刚才插入的用户的 user_id
SELECT id FROM ab_user WHERE username = 'test';

select * from ab_user ;

select * from ab_user_role ;

-- 假设返回的 user_id 为 123，那么插入用户角色关联
INSERT INTO ab_user_role (user_id, role_id)
VALUES (8, 7);

UPDATE ab_user
SET password = 'scrypt:32768:8:1$LisJovvQMQeyGIfW$b8bd8dd0a6b853f15618bb4f991ef49897437c5ea45821f40d09bb93f639a130d24b72b46cf9f6e552118e5196ebdc9bb3558b6c990c64ad714a4d2c4c39c6fc'
WHERE id = 8;

-- 密文：chenli26
-- scrypt:32768:8:1$LisJovvQMQeyGIfW$b8bd8dd0a6b853f15618bb4f991ef49897437c5ea45821f40d09bb93f639a130d24b72b46cf9f6e552118e5196ebdc9bb3558b6c990c64ad714a4d2c4c39c6fc
```

## superset更换图标logo

参考资料：[superset修改logo](https://blog.csdn.net/qq_39945938/article/details/121488722)

找到静态文件的路径

```bash
cd /data/app/conda/envs/superset/lib/python3.9/site-packages/superset/static/assets/images
# 把huangtang_logo.png图标上传
```

修改superset_config.py文件

```bash
# 更换logo
# 指定静态目录的路径
APP_ICON = "/static/assets/images/huangtang_logo.png"
# 自定义 Favicon
# FAVICON = "/static/assets/images/huangtang_favicon.png"  # 将 custom_favicon.ico 替换为你自己的文件路径
FAVICONS = [{"href": "/static/assets/images/huangtang_favicon.png"}]
# 修改导航栏左上角的 Superset 文字
APP_NAME = "梅州市人民医院BI系统"
```

重启superset

看能否访问静态文件

```bash
http://192.168.200.136:8787/static/assets/images/huangtang_logo.png
http://192.168.200.136:8787/static/assets/images/huangtang_favicon.png
```

## superset重置用户密码操作

脚本`reset_superset_password.sh`

```bash
#!/bin/bash

# 激活 Conda 环境
eval "$(conda shell.bash hook)"
conda activate superset

# 设置 Flask 环境变量
export FLASK_APP=superset
export SUPERSET_CONFIG_PATH=~/.superset/superset_config.py

# 提示用户输入用户名
read -p "请输入要重置密码的用户名: " username

# 检查用户名是否为空
if [ -z "$username" ]; then
  echo "错误：用户名不能为空！"
  exit 1
fi

# 设置新密码
new_password="Mzrmyy@0753"

# 执行密码重置命令，并捕获输出
output=$(flask fab reset-password --username "$username" --password "$new_password" 2>&1)

# 检查命令输出
if echo "$output" | grep -q "not found"; then
  echo "错误：用户 $username 不存在！"
  exit 1
elif echo "$output" | grep -q "reseted"; then
  echo "用户 $username 的密码已成功重置为 $new_password。"
else
  echo "密码重置失败，请检查错误信息。"
  echo "$output"
  exit 1
fi
```

使用

```bash
(superset) [root@superset script]# ./reset_superset_password.sh 
请输入要重置密码的用户名: test
用户 test 的密码已成功重置为 superset_0753。
(superset) [root@superset script]# ./reset_superset_password.sh 
请输入要重置密码的用户名: test26
错误：用户 test26 不存在！
(superset) [root@superset script]# 
```



## superset问题总结

1.如果修改了某个配置文件，然后重启没反应，然后查看状态superset是正在运行的，但是访问地址就是打不开。

```bash
conda activate superset

# 执行superset db upgrade提示以下内容
superset db upgrade
(superset) [root@superset script]# superset db upgrade
Usage: superset [OPTIONS] COMMAND [ARGS]...
Try 'superset --help' for help.

Error: Could not locate a Flask application. Use the 'flask --app' option, 'FLASK_APP' environment variable, or a 'wsgi.py' or 'app.py' file in the current directory.


# 执行这两个命令
conda activate superset
export FLASK_APP=superset # 设置flask_app环境变量
export SUPERSET_CONFIG_PATH=~/.superset/superset_config.py
# 再次执行更新
superset db upgrade

# 最后再启动就可以了
```

# DataEase部署

参考链接：[离线安装](https://dataease.io/docs/v2/installation/offline_INSTL_and_UPG/)

## 安装部署

1.解压安装包

```bash
tar -zxvf dataease-offline-installer-v2.10.9-ce.tar.gz -C /data/app/dataease
```

2.创建数据库

```bash
-- 进入MySQL命令行
[root@hadoop01]# mysql -u root -p 
Enter password: xxxxxx
-- 创建 dataease 数据库用户和密码，并限定登陆范围
mysql > CREATE USER 'dataease'@'%' IDENTIFIED BY '******';
-- 创建 dataease 的元数据，并指定编码
mysql > CREATE DATABASE dataease DEFAULT CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci;
-- 为dataease数据库授权
mysql > grant all privileges on dataease.* to 'dataease'@'%';
-- 刷新权限
mysql > flush privileges;
```

3.设置安装参数

```bash
[root@superset dataease-offline-installer-v2.10.9-ce]# pwd
/data/app/dataease/dataease-offline-installer-v2.10.9-ce
[root@superset dataease-offline-installer-v2.10.9-ce]# vim install.conf

# 基础配置
## 安装目录
DE_BASE=/data/app/dataease
## Service 端口
DE_PORT=8100
## 登录超时时间，单位min。如果不设置则默认8小时，也就是480
DE_LOGIN_TIMEOUT=480
## 安装模式，community | enterprise
DE_INSTALL_MODE=community
# 数据库配置
## 是否使用外部数据库
DE_EXTERNAL_MYSQL=true
## 数据库地址
DE_MYSQL_HOST=*******
## DataEase 数据库库名
DE_MYSQL_DB=dataease
## 数据库用户名
DE_MYSQL_USER=dataease
## 数据库密码
DE_MYSQL_PASSWORD=*******
## 数据库参数
DE_MYSQL_PARAMS="autoReconnect=false&useUnicode=true&characterEncoding=UTF-8&characterSetResults=UTF-8&zeroDateTimeBehavior=convertToNull&useSSL=false&allowPublicKeyRetrieval=true"
```

4.开启关闭服务

安装好是开启了，但是因为社区版的，只有管理员一个用户，不能创建用户，同时也不支持Hive数据源，决定放弃。

```bash
# 开启服务
dectl start
# 关闭服务
dectl stop
# 查看状态
dectl status
# 重新启动
dectl restart
```

# Doris集群安装

参考资料：[从0到1搭建Doris集群](https://www.yuque.com/yxiansheng-njx6f/uizabi/ezrtw7uxgk1d91vy)

## 1.基础环境搞定

配置主机名，网络映射，ssh免密登录配置，jdk安装（安装2.1.10稳定版本的doris，所以安装jdk 1.8可以）、gcc环境、设置系统参数、时钟同步等（参考前面的安装文档）

设置系统最大文件打开句柄数

```bash
1.打开文件
vim /etc/security/limits.conf 

2.在文件最后添加下面几行信息(注意* 也要复制进去)

* soft nofile 65536
* hard nofile 65536 
* soft nproc 65536
* hard nproc 65536

ulimit -n 65536 临时生效

修改完文件后需要重新启动虚拟机
重启永久生效，也可以用


如果不修改这个句柄数大于等于60000，回头启动doris的be节点的时候就会报如下的错
如果报错：Please set the maximum number of open file descriptors to be 65536 using 'ulimit -n 65536'.
代表句柄数没有生效，需要临时设置或者重启电脑


第一次启动的时候可能会报错
Please set vm.max_map_count to be 2000000 under root using 'sysctl -w vm.max_map_count=2000000'.
解决方案：
命令行输入：sysctl -w vm.max_map_count=2000000
```

分发文件到不同的服务器上

```bash
xsync.sh /etc/security/limits.conf
```

设置文件包含限制一个进程可以拥有VMA（虚拟内存区域）的数量

```bash
临时生效：
sysctl -w vm.max_map_count=2000000

永久生效
vim /etc/sysctl.conf
在文件最后一行添加
vm.max_map_count=2000000

让他永久生效
sysctl -p

检查是否生效
sysctl -a|grep vm.max_map_count
```

记得分发文件

```bash
xsync.sh /etc/sysctl.conf
```

## 2.FE部署

1.创建fe元数据存储的目录

```bash
mkdir -p /data/u01/app/doris/doris-meta/
```

2.上传文件并解压

```bash
# 上传到/data/u01/soft目录下，并解压到当前文件夹
tar -zxvf apache-doris-2.1.10-bin-x64.tar.gz 

# 移动文件
mv fe/ /data/u01/app/doris/
```

3.修改配置文件

```bash
# fe.conf文件：位于fe安装目录下的conf目录
cd /data/u01/app/doris/fe/conf
vim fe.conf

#配置文件中指定元数据路径： 注意这个文件夹要自己创建
meta_dir = /data/u01/app/doris/doris-meta
#修改绑定 ip（分发之后，每台机器修改成自己的 ip） 
priority_networks = 192.168.200.180/24
```

4.分区集群

```bash
xsync.sh doris -root
```

5.启动

```bash
cd /data/u01/app/doris/fe/bin
./start_fe.sh --daemon
# 其他机器也启动一下
[root@doris bin]# jps-all.sh 
==============查询当前所有服务器的jps情况==============
**************doris当前jps情况***************
4150 Jps
32587 DorisFE
**************doris1当前jps情况***************
1428 DorisFE
2411 Jps
**************doris2当前jps情况***************
26112 Jps
25732 DorisFE
=======================查询完毕========================
[root@doris bin]# 
```

## 3.BE部署

创建BE的存放目录（每个节点都要）

```bash
cd /data/u01/app/doris
mkdir doris-storage1
mkdir doris-storage2
```

移动be文件

```bash
cd /data/u01/soft/apache-doris-2.1.10-bin-x64
mv be /data/u01/app/doris/
```

进入到be的conf目录下修改配置文件

```bash
cd /data/u01/app/doris/be/conf
vim be.conf

#配置文件中指定数据存放路径： 
storage_root_path = /data/u01/app/doris/doris-storage1;/data/u01/app/doris/doris-storage2

#修改绑定 ip（每台机器修改成自己的 ip） 
priority_networks = 192.168.200.180/24

第一次启动的时候可能会报错
Please set vm.max_map_count to be 2000000 under root using 'sysctl -w vm.max_map_count=2000000'.
解决方案：
命令行输入：sysctl -w vm.max_map_count=2000000

如果报错：Please set the maximum number of open file descriptors to be 65536 using 'ulimit -n 65536'.
```

be先不启动，因为fe和be是两个单独的个体，他们还不认识，就需要我们通过mysql客户端将他们建立起联系。如果没装mysql，记得先装mysql。

使用mysql client连接FE

```bash
mysql -h doris -P 9030 -uroot

第一次没有密码可以进入。
```

查看BE状态

```bash
SHOW PROC '/backends'\G;

Alive 为 false 表示该 BE 节点还是死的
```

前面已经说过，默认root用户无密码，通过以下命令修改doris_0753密码。

```bash
mysql> SET PASSWORD FOR 'root' = PASSWORD('doris_0753');
```

添加BE

```bash
mysql> ALTER SYSTEM ADD BACKEND "doris:9050";
mysql> ALTER SYSTEM ADD BACKEND "doris1:9050";
mysql> ALTER SYSTEM ADD BACKEND "doris2:9050";
```

启动BE

```bash
启动 BE（每个节点） 
/data/u01/app/doris/be/bin/start_be.sh --daemon

启动后再次查看BE的节点
mysql -h doris -P 9030 -uroot -pdoris_0753
SHOW PROC '/backends'\G; 
Alive 为 true 表示该 BE 节点存活。
```

这里显示doris1和doris2没有启动成功，Alive: false，端口号也显示未开启

```bash
mysql> SHOW PROC '/backends'\G; 
*************************** 1. row ***************************
              BackendId: 26338
                   Host: doris
          HeartbeatPort: 9050
                 BePort: 9060
               HttpPort: 8040
               BrpcPort: 8060
     ArrowFlightSqlPort: -1
          LastStartTime: 2025-07-04 14:50:32
          LastHeartbeat: 2025-07-04 14:54:40
                  Alive: true
   SystemDecommissioned: false
              TabletNum: 22
       DataUsedCapacity: 0.000 
      TrashUsedCapacity: 0.000 
          AvailCapacity: 1.932 TB
          TotalCapacity: 1.952 TB
                UsedPct: 1.04 %
         MaxDiskUsedPct: 1.04 %
     RemoteUsedCapacity: 0.000 
                    Tag: {"location" : "default"}
                 ErrMsg: 
                Version: doris-2.1.10-rc01-33df5ba180
                 Status: {"lastSuccessReportTabletsTime":"2025-07-04 14:53:53","lastStreamLoadTime":-1,"isQueryDisabled":false,"isLoadDisabled":false}
HeartbeatFailureCounter: 0
               NodeRole: mix
*************************** 2. row ***************************
              BackendId: 26357
                   Host: doris1
          HeartbeatPort: 9050
                 BePort: -1
               HttpPort: -1
               BrpcPort: -1
     ArrowFlightSqlPort: -1
          LastStartTime: NULL
          LastHeartbeat: NULL
                  Alive: false
   SystemDecommissioned: false
              TabletNum: 0
       DataUsedCapacity: 0.000 
      TrashUsedCapacity: 0.000 
          AvailCapacity: 1.000 B
          TotalCapacity: 0.000 
                UsedPct: 0.00 %
         MaxDiskUsedPct: 0.00 %
     RemoteUsedCapacity: 0.000 
                    Tag: {"location" : "default"}
                 ErrMsg: java.net.NoRouteToHostException: 没有到主机的路由 (Host unreachable)
                Version: 
                 Status: {"lastSuccessReportTabletsTime":"N/A","lastStreamLoadTime":-1,"isQueryDisabled":false,"isLoadDisabled":false}
HeartbeatFailureCounter: 35
               NodeRole: 
*************************** 3. row ***************************
              BackendId: 26376
                   Host: doris2
          HeartbeatPort: 9050
                 BePort: -1
               HttpPort: -1
               BrpcPort: -1
     ArrowFlightSqlPort: -1
          LastStartTime: NULL
          LastHeartbeat: NULL
                  Alive: false
   SystemDecommissioned: false
              TabletNum: 0
       DataUsedCapacity: 0.000 
      TrashUsedCapacity: 0.000 
          AvailCapacity: 1.000 B
          TotalCapacity: 0.000 
                UsedPct: 0.00 %
         MaxDiskUsedPct: 0.00 %
     RemoteUsedCapacity: 0.000 
                    Tag: {"location" : "default"}
                 ErrMsg: java.net.NoRouteToHostException: 没有到主机的路由 (Host unreachable)
                Version: 
                 Status: {"lastSuccessReportTabletsTime":"N/A","lastStreamLoadTime":-1,"isQueryDisabled":false,"isLoadDisabled":false}
HeartbeatFailureCounter: 35
               NodeRole: 
3 rows in set (0.02 sec)

ERROR: 
No query specified

```

这是由于防火墙导致的，把防火墙关闭了

```bash
systemctl stop firewalld
systemctl status firewalld
```

## 4.查看前端页面

```bash
http://doris:8030
账号：root 密码:doris_0753
```



# 问题总结

## 1.Permission denied: user=dr.who, access=WRITE, inode="/":hadoop:supergroup:drwxr-xr-x

当前用户权限不足，所以无法操作/目录

解决方案：给相应的目录赋予权限

```shell
hdfs dfs -chmod -R 777 /    --直接赋予根目录的权限
hdfs dfs -chmod -R 777 /test --赋予对应目录的权限
```

[解决方案](https://blog.csdn.net/weixin_42656458/article/details/113176108)

创建路径，赋予权限。

```bash
[hadoop@cesdb jars]$ hadoop fs -mkdir -p /user/hive/warehouse/ods.db/cdr/ods_cdr_lis_indicators_mi
[hadoop@cesdb jars]$ hadoop fs -chmod -R 777 /user/hive/warehouse/ods.db/cdr/ods_cdr_lis_indicators_mi
```



## 2.错误: 找不到或无法加载主类 org.apache.hadoop.mapreduce.v2.app.MRAppMaster

说明yarn没有正确配置

需要添加以下配置

```
<property>
    <name>yarn.application.classpath</name>
<value>$HADOOP_HOME/etc/hadoop:$HADOOP_HOME//share/hadoop/common/lib/*:$HADOOP_HOME//share/hadoop/common/*:$HADOOP_HOME//share/hadoop/hdfs:$HADOOP_HOME//share/hadoop/hdfs/lib/*:$HADOOP_HOME//share/hadoop/hdfs/*:$HADOOP_HOME//share/hadoop/mapreduce/lib/*:$HADOOP_HOME//share/hadoop/mapreduce/*:$HADOOP_HOME//share/hadoop/yarn:$HADOOP_HOME//share/hadoop/yarn/lib/*:$HADOOP_HOME//share/hadoop/yarn/*:/data/u01/app/hive/apache-hive-3.1.3/lib/*</value>
  </property>
```

## 3.配置Map任务时未能找到`org.apache.hive.hcatalog.data.JsonSerDe`类

配置Map任务时未能找到`org.apache.hive.hcatalog.data.JsonSerDe`类

第一种解决方案：

执行Hive时，手动添加jar包

```
ADD JAR $HIVE_HOME/lib/hive-hcatalog-core-3.1.3.jar;
```

第二种解决方案：

配置hive-site.xml

```
<property>
  <name>hive.aux.jars.path</name>
  <value>/user/hive/lib/hcatalog-core.jar</value>
</property>
```

配置yarn-site.xml

```
把hive的jar包路径添加到yarn-site.xml的classpath中
/data/u01/app/hive/apache-hive-3.1.3/lib/*


<property>
    <name>yarn.application.classpath</name>
<value>$HADOOP_HOME/etc/hadoop:$HADOOP_HOME//share/hadoop/common/lib/*:$HADOOP_HOME//share/hadoop/common/*:$HADOOP_HOME//share/hadoop/hdfs:$HADOOP_HOME//share/hadoop/hdfs/lib/*:$HADOOP_HOME//share/hadoop/hdfs/*:$HADOOP_HOME//share/hadoop/mapreduce/lib/*:$HADOOP_HOME//share/hadoop/mapreduce/*:$HADOOP_HOME//share/hadoop/yarn:$HADOOP_HOME//share/hadoop/yarn/lib/*:$HADOOP_HOME//share/hadoop/yarn/*:/data/u01/app/hive/apache-hive-3.1.3/lib/*</value>
  </property>
```

