# 大数据集群优化

# 资源配置优化

## yarn-site.xml

```xml
<configuration>
    <!-- NodeManager的总内存和CPU核心数 -->
    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>131072</value> <!-- 每台机器的总内存：24GB,现在调整为128G -->
    </property>
    <property>
        <name>yarn.nodemanager.resource.cpu-vcores</name>
        <value>64</value> <!-- 每台机器的CPU核心数：16 ，现在调整为64核-->
    </property>

    <!-- YARN调度器每个容器的内存和CPU核心分配 -->
    <property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>2048</value> <!-- 单个容器最小内存：1GB，调整为2GB -->
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>32768</value> <!-- 单个容器最大内存：8GB，调整为32GB -->
    </property>
    <property>
        <name>yarn.scheduler.minimum-allocation-vcores</name>
        <value>1</value> <!-- 单个容器最小CPU核心数：2，调整为1核，更加灵活 -->
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-vcores</name>
        <value>16</value> <!-- 单个容器最大CPU核心数：4，调整为16核 -->
    </property>
    
    <!--启用资源动态分配（如启用 Shuffle Service）。为了更好地支持资源动态分配，建议启用 YARN Shuffle Service-->
    <property>
    	<name>yarn.nodemanager.aux-services</name>
    	<value>spark_shuffle</value>
	</property>
	<property>
    	<name>yarn.nodemanager.aux-services.spark_shuffle.class</name>
    	<value>org.apache.spark.network.yarn.YarnShuffleService</value>
	</property>


    <!-- 启用LinuxContainerExecutor和CGroups支持 -->
    <property>
        <name>yarn.nodemanager.container-executor.class</name>
        <value>org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor</value>
    </property>
    <property>
        <name>yarn.nodemanager.linux-container-executor.cgroups.enabled</name>
        <value>true</value>
    </property>
</configuration>

```

## mapred-site.xml

```xml
<configuration>
    <!-- Mapper和Reducer的内存和CPU核心配置 -->
    <property>
        <name>mapreduce.map.memory.mb</name>
        <value>8192</value> <!-- 每个Mapper任务的内存：4GB，调整为8GB -->
    </property>
    <property>
        <name>mapreduce.reduce.memory.mb</name>
        <value>8192</value> <!-- 每个Reducer任务的内存：4GB，调整为8GB -->
    </property>
    <property>
        <name>mapreduce.map.cpu.vcores</name>
        <value>4</value> <!-- 每个Mapper任务使用2个核心，调整为4核 -->
    </property>
    <property>
        <name>mapreduce.reduce.cpu.vcores</name>
        <value>4</value> <!-- 每个Reducer任务使用2个核心，调整为4核 -->
    </property>
    
    <!-- Mapper和Reducer的JVM堆大小 -->
    <property>
        <name>mapreduce.map.java.opts</name>
        <value>-Xmx6144m -XX:+UseG1GC</value> <!-- Mapper的JVM堆大小：3GB，调整为6GB -->
    </property>
    <property>
        <name>mapreduce.reduce.java.opts</name>
        <value>-Xmx6144m -XX:+UseG1GC</value> <!-- Reducer的JVM堆大小：3GB，调整为6GB -->
    </property>
</configuration>

```

## hadoop-env.sh

```bash
# 设置Hadoop的全局内存和GC配置
export HADOOP_HEAPSIZE=4096  # Hadoop全局内存：4GB
export HADOOP_NAMENODE_OPTS="-Xmx2048m -XX:+UseG1GC"  # NameNode的堆内存：2GB
# export HADOOP_DATANODE_OPTS="-Xmx4096m -XX:+UseG1GC"  # DataNode的堆内存：4GB
export HDFS_DATANODE_OPTS="$HDFS_DATANODE_OPTS -Xmx4096m -XX:+UseG1GC"
export HADOOP_OPTS="-XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=35"  # 启用G1GC
# export HDFS_DATANODE_OPTS="$HDFS_DATANODE_OPTS -Xmx4096m -XX:+UseG1GC"


```

## yarn-env.sh

```bash
# 设置YARN的内存和垃圾回收策略
export HADOOP_HEAPSIZE=4096  # YARN的全局内存：4GB
export YARN_RESOURCEMANAGER_OPTS="-Xmx4096m -XX:+UseG1GC"  # ResourceManager内存：4GB
export YARN_NODEMANAGER_OPTS="-Xmx4096m -XX:+UseG1GC"  # NodeManager内存：4GB
export HADOOP_OPTS="-XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=35"
```

## hive-env.sh

```bash
# 设置Hive服务的内存配置
export HIVE_HEAPSIZE=4096  # HiveServer2的堆内存：4GB
export HADOOP_HEAPSIZE=4096  # Hive使用的Hadoop客户端内存：4GB
export HADOOP_OPTS="-XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=35"  # 启用G1GC
```



# 资源调度策略的优化

以下是关于 YARN 的三种调度策略：FIFO 调度器、容量调度器、公平调度器的定义、应用场景、优点及缺点：

| **调度策略**    | **定义**                                                     | **应用场景**                                                 | **优点**                                                     | **缺点**                                                     |
| --------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **FIFO 调度器** | 先来先服务的调度策略，根据作业提交的时间顺序分配资源，直到作业完成或资源耗尽。 | 适用于作业较少、任务时间较短的场景，或者对任务完成顺序有严格要求的环境。 | 1. 简单易于实现<br>2. 适合短任务队列                         | 1. 不适合资源竞争环境<br>2. 后续作业可能长期等待             |
| **容量调度器**  | 将集群划分为多个队列，每个队列分配一定比例的资源，队列内部使用 FIFO 调度。 | 适用于多租户环境，不同团队或部门共享集群，资源需求明确分配比例。 | 1. 支持多租户共享<br>2. 保证资源利用率<br>3. 支持动态调整队列容量 | 1. 配置复杂<br>2. 若队列资源不足，可能导致作业延迟           |
| **公平调度器**  | 为所有任务公平地分配资源，确保资源长期均衡分配，支持任务抢占和动态调整。 | 适用于共享集群中任务大小不均、不同任务需要资源动态调整的场景。 | 1. 提高资源利用率<br>2. 支持动态资源调整<br>3. 适合混合任务环境 | 1. 任务可能被抢占，影响优先级<br>2. 需要较强的配置和调优能力 |



**容量调度器与公平调度器的对比**

| **特点**         | **容量调度器（Capacity Scheduler）**                         | **公平调度器（Fair Scheduler）**                       |
| ---------------- | ------------------------------------------------------------ | ------------------------------------------------------ |
| **资源分配方式** | 按预设的静态容量分配（如 40% / 30% / 30%）。                 | 按权重动态分配资源，空闲资源可以灵活调整。             |
| **动态资源借用** | 空闲资源可以动态借用，但需要手动启用“资源回收”和“抢占机制”。 | 默认支持动态资源调整和抢占机制，调整更灵活。           |
| **配置复杂度**   | 配置子队列的容量比较直观（基于百分比），更适合固定分配资源的场景。 | 配置基于权重，适合多样化任务和动态负载的场景。         |
| **优先级支持**   | 可支持任务优先级，但配置稍复杂。                             | 自带抢占功能，适合优先处理短任务或小任务。             |
| **适用场景**     | 多租户场景，队列容量固定分配，比如部门或团队资源隔离。       | 灵活的资源分配场景，比如频繁变化的任务和动态负载环境。 |

## yarn-site.xml

```xml
<configuration>
    <!-- 使用容量调度器 -->
    <property>
        <name>yarn.resourcemanager.scheduler.class</name>
        <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</value>
    </property>

    <!-- NodeManager 的总内存和 CPU 核数 -->
    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>119016</value> <!-- 每台机器分配 116 GB 内存 -->
    </property>
    <property>
        <name>yarn.nodemanager.resource.cpu-vcores</name>
        <value>56</value> <!-- 每台机器分配 56 核 -->
    </property>

    <!-- 每个容器的最小和最大资源 -->
    <property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>1024</value> <!-- 单容器最小 1 GB 内存 -->
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>16384</value> <!-- 单容器最大 16 GB 内存 -->
    </property>
    <property>
        <name>yarn.scheduler.minimum-allocation-vcores</name>
        <value>1</value> <!-- 单容器最小 1 核 -->
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-vcores</name>
        <value>8</value> <!-- 单容器最大 8 核 -->
    </property>

    <!-- 启用抢占机制 -->
    <property>
        <name>yarn.resourcemanager.scheduler.monitor.enable</name>
        <value>true</value>
    </property>

    <!-- 启用 Dominant Resource Calculator -->
    <property>
        <name>yarn.scheduler.capacity.resource-calculator</name>
        <value>org.apache.hadoop.yarn.util.resource.DominantResourceCalculator</value>
    </property>
</configuration>

```

## capacity-scheduler.xml

```xml
<configuration>

  <!-- 设置根队列 -->
  <property>
    <name>yarn.scheduler.capacity.root.queues</name>
    <value>default</value>
    <description>定义根队列下的子队列</description>
  </property>

  <!-- 设置 default 队列 -->
  <property>
    <name>yarn.scheduler.capacity.root.default.queues</name>
    <value>queue1,queue2,queue3</value>
    <description>default 队列下的子队列</description>
  </property>

  <!-- 设置 queue1 队列 -->
  <property>
    <name>yarn.scheduler.capacity.root.default.queue1.capacity</name>
    <value>40</value>
    <description>queue1 的初始容量为 40%</description>
  </property>
  <property>
    <name>yarn.scheduler.capacity.root.default.queue1.maximum-capacity</name>
    <value>100</value>
    <description>queue1 的最大容量为 100%</description>
  </property>
  <property>
    <name>yarn.scheduler.capacity.root.default.queue1.user-limit-factor</name>
    <value>2</value>
    <description>单用户在 queue1 队列中可使用的最大资源倍数</description>
  </property>
  <property>
    <name>yarn.scheduler.capacity.root.default.queue1.state</name>
    <value>RUNNING</value>
    <description>queue1 的状态设置为运行</description>
  </property>

  <!-- 设置 queue2 队列 -->
  <property>
    <name>yarn.scheduler.capacity.root.default.queue2.capacity</name>
    <value>30</value>
    <description>queue2 的初始容量为 30%</description>
  </property>
  <property>
    <name>yarn.scheduler.capacity.root.default.queue2.maximum-capacity</name>
    <value>100</value>
    <description>queue2 的最大容量为 100%</description>
  </property>
  <property>
    <name>yarn.scheduler.capacity.root.default.queue2.user-limit-factor</name>
    <value>2</value>
    <description>单用户在 queue2 队列中可使用的最大资源倍数</description>
  </property>
  <property>
    <name>yarn.scheduler.capacity.root.default.queue2.state</name>
    <value>RUNNING</value>
    <description>queue2 的状态设置为运行</description>
  </property>

  <!-- 设置 queue3 队列 -->
  <property>
    <name>yarn.scheduler.capacity.root.default.queue3.capacity</name>
    <value>30</value>
    <description>queue3 的初始容量为 30%</description>
  </property>
  <property>
    <name>yarn.scheduler.capacity.root.default.queue3.maximum-capacity</name>
    <value>100</value>
    <description>queue3 的最大容量为 100%</description>
  </property>
  <property>
    <name>yarn.scheduler.capacity.root.default.queue3.user-limit-factor</name>
    <value>2</value>
    <description>单用户在 queue3 队列中可使用的最大资源倍数</description>
  </property>
  <property>
    <name>yarn.scheduler.capacity.root.default.queue3.state</name>
    <value>RUNNING</value>
    <description>queue3 的状态设置为运行</description>
  </property>

  <!-- 设置抢占机制 -->
  <property>
    <name>yarn.resourcemanager.scheduler.monitor.enable</name>
    <value>true</value>
    <description>启用资源抢占机制</description>
  </property>
  <property>
    <name>yarn.resourcemanager.scheduler.monitor.policies</name>
    <value>org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy</value>
    <description>使用容量调度器的抢占策略</description>
  </property>
  <property>
    <name>yarn.resourcemanager.monitor.capacity.preemption.monitoring-interval</name>
    <value>3000</value>
    <description>抢占监控间隔为 3000 毫秒</description>
  </property>
  <property>
    <name>yarn.resourcemanager.monitor.capacity.preemption.max-allowable-limit</name>
    <value>0.5</value>
    <description>抢占的最大可用资源比例为 50%.如果集群负载高且任务长时间占用资源，可以设置较大的比例（如 0.7）如果任务对稳定性要求较高，适合设置为较低的比例（如 0.3）。</description>
  </property>
  <property>
    <name>yarn.resourcemanager.monitor.capacity.preemption.total-preemption-per-round</name>
    <value>0.1</value>
    <description>每轮抢占的资源比例为 10%.如果需要快速调整资源分配，可以适当提高比例（如 0.2 或 0.3）.如果希望抢占更平缓，减少任务中断的影响，建议保持较低的比例（如 0.1）.</description>
  </property>

  <!-- 动态资源分配 -->
  <property>
    <name>yarn.scheduler.capacity.resource-calculator</name>
    <value>org.apache.hadoop.yarn.util.resource.DominantResourceCalculator</value>
    <description>基于主导资源（CPU 和内存）的动态资源分配</description>
  </property>
  <property>
    <name>yarn.scheduler.capacity.node-locality-delay</name>
    <value>40</value>
    <description>节点本地化调度延迟，单位为调度机会数</description>
  </property>

</configuration>

```

## hive连接

在 Hive 的配置文件 `hive-site.xml` 中，可以设置默认的 YARN 队列，同时支持用户动态指定队列。

**1.为用户分配固定队列**

在hive-site.xml文件中，添加以下配置

```xml
<property>
  <name>mapreduce.job.queuename</name>
  <value>queue1</value>
</property>

<property>
  <name>tez.queue.name</name>
  <value>queue1</value>
</property>

```

使用时设置队列

```sql
set mapreduce.job.queuename=queue1;
set mapreduce.job.queuename=queue2;
set mapreduce.job.queuename=queue3;
```

## 配置解释

以下是对这几个配置项的详细解释，帮助你理解它们的含义和作用：

---

### **1. 单用户在 queue1 队列中可使用的最大资源倍数**

#### 配置项：
```xml
<property>
  <name>yarn.scheduler.capacity.root.default.queue1.user-limit-factor</name>
  <value>2</value>
</property>
```

#### **含义**：
- 表示单个用户在队列中最多可以使用的资源倍数。
- 默认情况下，队列中的资源会按照多个用户的公平份额进行分配。例如，`queue1` 的容量为 40%，如果有两个用户，每个用户默认可以使用 `40% / 2 = 20%` 的资源。
- 如果用户需要更多资源，而其他用户的资源未用满，则可以使用额外的资源。`user-limit-factor=2` 表示该用户最多可以使用自己份额的 **2 倍**，即最多可以使用 `40% × 2 = 80%` 的资源。

#### **作用**：
- 限制某个用户在队列中的资源独占，避免资源争抢导致其他用户的任务被饿死。
- 允许在其他用户不活跃时动态分配资源，提高资源利用率。

#### **场景**：
- 多用户共享队列时，设置 `user-limit-factor` 可以动态调整用户资源上限。

---

### **2. 抢占监控间隔为 3000 毫秒**

#### 配置项：
```xml
<property>
  <name>yarn.resourcemanager.monitor.capacity.preemption.monitoring-interval</name>
  <value>3000</value>
</property>
```

#### **含义**：
- 指 ResourceManager 检查和执行抢占操作的时间间隔，单位为毫秒。
- 每隔 3000 毫秒（3 秒），ResourceManager 会检查所有队列的资源使用情况，判断是否需要执行资源抢占。

#### **作用**：
- 提高资源动态分配效率，确保资源能够及时回收并重新分配给需要的队列。
- 间隔时间过长会导致抢占反应慢，过短可能增加 ResourceManager 的负担。

#### **场景**：
- 如果任务负载变化频繁（任务提交和完成速度快），可以适当减少间隔时间，比如设置为 1000 毫秒。
- 如果集群负载变化较慢，3 秒是一个推荐的默认值。

---

### **3. 抢占的最大可用资源比例为 50%**

#### 配置项：
```xml
<property>
  <name>yarn.resourcemanager.monitor.capacity.preemption.max-allowable-limit</name>
  <value>0.5</value>
</property>
```

#### **含义**：
- 表示在抢占资源时，最多允许抢占的资源比例。
- 如果 `max-allowable-limit=0.5`，意味着抢占时最多可以占用当前集群总资源的 50%。

#### **作用**：
- 限制抢占行为的影响范围，避免过多资源被抢占导致其他队列的任务大规模被中断。
- 提供抢占的上限，平衡资源分配和任务稳定性。

#### **场景**：
- 如果集群负载高且任务长时间占用资源，可以设置较大的比例（如 0.7）。
- 如果任务对稳定性要求较高，适合设置为较低的比例（如 0.3）。

---

### **4. 每轮抢占的资源比例为 10%**

#### 配置项：
```xml
<property>
  <name>yarn.resourcemanager.monitor.capacity.preemption.total-preemption-per-round</name>
  <value>0.1</value>
</property>
```

#### **含义**：
- 表示每轮抢占中，最多允许抢占资源的比例。
- 如果 `total-preemption-per-round=0.1`，意味着每次抢占时，最多占用当前集群总资源的 10%。

#### **作用**：
- 限制每轮抢占资源的规模，避免过于激进的抢占导致任务频繁中断。
- 提供逐步调整资源分配的机制，提高抢占的平稳性。

#### **场景**：
- 如果需要快速调整资源分配，可以适当提高比例（如 0.2 或 0.3）。
- 如果希望抢占更平缓，减少任务中断的影响，建议保持较低的比例（如 0.1）。

---

### **总结**

| 配置项                     | 含义                                                 | 作用                                                         | 建议设置                  |
| -------------------------- | ---------------------------------------------------- | ------------------------------------------------------------ | ------------------------- |
| **单用户最大资源倍数**     | 限制单用户最多使用资源的倍数，动态调整用户资源上限。 | 防止资源独占，同时允许用户在其他用户资源未使用时扩展其任务资源。 | 2（推荐），视用户需求调整 |
| **抢占监控间隔**           | ResourceManager 检查和执行抢占的间隔时间（毫秒）。   | 平衡抢占响应速度与系统开销，建议保持 3 秒为默认值。          | 3000（默认）              |
| **抢占的最大可用资源比例** | 限制抢占行为对整个集群资源的影响，避免抢占范围过大。 | 平衡抢占和任务稳定性，适合设置为 50% 或更高以提高资源利用率。 | 0.5（推荐）               |
| **每轮抢占的资源比例**     | 限制每轮抢占资源的规模，逐步调整资源分配。           | 避免过于激进的抢占，建议设置较低值（如 10%），视集群需要动态调整。 | 0.1（推荐）               |

如果你的任务负载变化较快，可以考虑缩短监控间隔和增加抢占比例。如果任务对稳定性要求较高，建议保持以上推荐设置，确保平稳运行。



# 大任务优化策略

从截图来看，这种 **800+ Map 和 900+ Reduce** 的任务确实属于 **大任务**，主要原因如下：

---

### **1. 任务划分的规模**
- **Map 阶段任务数（804 个）**：
  - 每个 Mapper 任务通常对应输入数据的一个逻辑分片（split）。
  - 如果输入数据非常大（数百 GB 或 TB），分片数会增加，从而导致大量的 Map 任务。

- **Reduce 阶段任务数（904 个）**：
  - Reduce 任务数量取决于数据分区数（partitions）。默认情况下，分区数与 Reducer 数量一致。
  - 如果有大量的键值对需要分组和聚合，Reduce 数量通常会较多。

因此，这种任务划分表明输入数据量很大，或者 Reduce 需要处理的数据分区非常多。

---

### **2. 是否算大任务？**
**判断任务是否为“大任务”**，通常需要结合以下几个因素：

#### **2.1 输入数据量**
- 如果任务的输入数据量在 **TB 级别**（通常需要数百个 Map 和 Reduce 任务处理），可被视为大任务。
- 对应的输入数据可以通过以下方法查看：
  - **HDFS 文件总大小**（`hadoop fs -du -h`）。
  - **每个 split 的大小**：Mapper 任务数与输入分片数（split）有关。

#### **2.2 Map 和 Reduce 数量**
- 一般来说：
  - **小任务**：< 100 个 Mapper 或 Reducer。
  - **中等任务**：100-500 个 Mapper 或 Reducer。
  - **大任务**：> 500 个 Mapper 或 Reducer。
- 当前任务的规模（804 个 Mapper 和 904 个 Reducer）属于较大任务。

#### **2.3 运行时间**
- 如果任务需要 **运行数小时甚至更长时间**，通常被视为大任务。
- 当前任务已经运行了 10 分钟，但只有少量任务完成，表明任务数据量较大，处理时间可能会更长。

#### **2.4 并发和资源需求**
- 大量的 Map 和 Reduce 任务会占用集群的内存和 CPU 资源。如果集群资源不足，任务容易进入 Pending 状态。

---

### **3. 大任务的优化建议**
对于这种任务，以下优化措施可以提升性能并减少资源消耗：

#### **3.1 优化 Mapper 数量**
- **调整输入分片大小**：
  - Mapper 数量取决于输入分片（split）的大小，默认情况下为 128 MB（`dfs.blocksize`）。
  - 如果输入文件较多或较小，可能会导致 Mapper 数量过多。
  - **解决方法**：增大分片大小，例如设置为 256 MB 或 512 MB：
    ```xml
    <property>
      <name>mapreduce.input.fileinputformat.split.minsize</name>
      <value>268435456</value> <!-- 256 MB -->
    </property>
    ```

#### **3.2 优化 Reducer 数量**
- **减少 Reduce 任务数**：
  - 当前 Reducer 数量（904 个）可能过高，导致资源浪费或任务调度延迟。
  - 通过设置 `mapreduce.job.reduces` 来减少 Reducer 数量。例如：
    ```bash
    set mapreduce.job.reduces=300;
    ```
  - **注意**：减少 Reducer 数量可能增加单个 Reducer 的负载，因此需要根据实际数据量权衡。

#### **3.3 启用中间数据压缩**
- 压缩 Mapper 输出可以减少 Shuffle 阶段的内存和网络消耗：
  ```xml
  <property>
    <name>mapreduce.map.output.compress</name>
    <value>true</value>
  </property>
  <property>
    <name>mapreduce.map.output.compress.codec</name>
    <value>org.apache.hadoop.io.compress.SnappyCodec</value>
  </property>
  ```

#### **3.4 合并小文件**
- 如果任务的输入数据是大量小文件，Map 数量可能会激增。可以通过将小文件合并为较大的文件来减少 Map 数量：
  - 使用 HDFS 自带的工具合并小文件：
    ```bash
    hadoop fs -getmerge /input_dir /output_file
    hadoop fs -put /output_file /new_input_dir
    ```

#### **3.5 提高并行度**
- **增加集群资源**：
  - 如果集群资源不足，考虑增加节点，提升并发能力。
- **增加容器数量**：
  - 修改 YARN 的 NodeManager 配置，增加容器并发数：
    ```xml
    <property>
      <name>yarn.nodemanager.resource.cpu-vcores</name>
      <value>64</value>
    </property>
    <property>
      <name>yarn.nodemanager.resource.memory-mb</name>
      <value>128000</value> <!-- 调整为实际可用内存 -->
    </property>
    ```

---

### **4. 需要注意的问题**
- **Pending 任务**：如果任务规模大且资源不足，许多任务可能长时间处于 Pending 状态。
- **内存不足**：大任务容易因 Mapper 或 Reducer 内存不足而失败（`OutOfMemoryError`）。
- **网络瓶颈**：Shuffle 阶段的数据传输可能成为性能瓶颈，尤其是 Reducer 数量较多时。

---

### **5. 总结**
- 当前任务（804 个 Mapper，904 个 Reducer）确实属于 **大任务**。
- 如果任务执行效率较低或资源消耗过高，建议通过调整分片大小、减少 Reducer 数量、启用压缩等方式优化任务。
- 如果集群资源不足，可以通过扩展资源或调整 YARN 配置提升并行能力。

你可以尝试上述优化措施，如果还有其他问题，可以随时交流！



在Hive中，可以通过设置相关参数动态调整分片大小、控制Map和Reduce的输入/输出数据量以及启用中间压缩。以下是相关配置的详细说明和设置方法：

---

### 1. **动态设置分片大小**
Hive作业分片由Hadoop的 `InputSplit` 决定，主要受以下参数影响：

#### **参数配置**
- **`mapreduce.input.fileinputformat.split.maxsize`**  
  设置每个分片的最大大小（以字节为单位）。较小的值会增加Map任务数。
  ```sql
  set mapreduce.input.fileinputformat.split.maxsize=128000000; -- 128MB
  ```

- **`mapreduce.input.fileinputformat.split.minsize`**  
  设置每个分片的最小大小。
  ```sql
  set mapreduce.input.fileinputformat.split.minsize=64000000; -- 64MB
  ```

- **`hive.input.format`**  
  控制分片的InputFormat，默认使用 `org.apache.hadoop.hive.ql.io.HiveInputFormat`，也可以切换到 `CombineHiveInputFormat` 来减少小文件问题。
  ```sql
  set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
  ```

---

### 2. **控制Map和Reduce任务的数据量**
可以通过以下参数控制单个Map和Reduce任务的输入/输出数据量：

#### **控制Map任务的数据量**
- **`mapreduce.map.memory.mb`**  
  设置每个Map任务的内存大小：
  ```sql
  set mapreduce.map.memory.mb=2048; -- 2GB
  ```

- **`mapreduce.map.java.opts`**  
  为Map任务分配JVM堆内存：
  ```sql
  set mapreduce.map.java.opts=-Xmx1536m; -- 设置为1.5GB
  ```

#### **控制Reduce任务的数据量**
- **`hive.exec.reducers.bytes.per.reducer`**  
  设置每个Reduce任务处理的字节数，默认值是64MB：
  ```sql
  set hive.exec.reducers.bytes.per.reducer=256000000; -- 256MB
  ```

- **`hive.exec.reducers.max`**  
  设置Reduce任务的最大数量：
  ```sql
  set hive.exec.reducers.max=50;
  ```

- **`mapreduce.reduce.memory.mb`**  
  设置每个Reduce任务的内存大小：
  ```sql
  set mapreduce.reduce.memory.mb=4096; -- 4GB
  ```

- **`mapreduce.reduce.java.opts`**  
  为Reduce任务分配JVM堆内存：
  ```sql
  set mapreduce.reduce.java.opts=-Xmx3072m; -- 设置为3GB
  ```

---

### 3. **启用中间数据压缩**
启用中间数据压缩可以减少Map和Reduce之间的网络传输量，提高性能。

#### **启用Map输出压缩**
- **`mapreduce.map.output.compress`**  
  启用Map阶段的输出压缩：
  ```sql
  set mapreduce.map.output.compress=true;
  ```

- **`mapreduce.map.output.compress.codec`**  
  设置Map输出压缩的编解码器（例如Snappy或Gzip）：
  ```sql
  set mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec;
  ```

#### **启用Reduce输出压缩**
- **`hive.exec.compress.output`**  
  启用Reduce阶段的输出压缩：
  ```sql
  set hive.exec.compress.output=true;
  ```

- **`mapreduce.output.fileoutputformat.compress`**  
  设置输出文件的压缩：
  ```sql
  set mapreduce.output.fileoutputformat.compress=true;
  ```

- **`mapreduce.output.fileoutputformat.compress.codec`**  
  设置输出文件压缩的编解码器：
  ```sql
  set mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.SnappyCodec;
  ```

- **`mapreduce.output.fileoutputformat.compress.type`**  
  设置压缩类型（`BLOCK`或`RECORD`）：
  ```sql
  set mapreduce.output.fileoutputformat.compress.type=BLOCK;
  ```

---

### 4. **其他性能优化相关参数**
- **`hive.auto.convert.join`**  
  启用自动MapJoin优化：
  ```sql
  set hive.auto.convert.join=true;
  ```

- **`hive.optimize.sort.dynamic.partition`**  
  启用动态分区排序优化：
  
  ```sql
  set hive.optimize.sort.dynamic.partition=true;
  ```
  
- **`hive.execution.engine`**  
  确保使用Tez或Spark引擎（比MR更高效）：
  ```sql
  set hive.execution.engine=tez;
  ```

---

### 示例：动态设置多个参数
以下是一个综合配置示例：

```sql
set mapreduce.input.fileinputformat.split.maxsize=256000000; -- 256MB
set mapreduce.input.fileinputformat.split.minsize=128000000; -- 128MB
set hive.exec.reducers.bytes.per.reducer=512000000; -- 512MB
set mapreduce.map.output.compress=true;
set mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec;
set hive.exec.compress.output=true;
set mapreduce.output.fileoutputformat.compress=true;
set mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.SnappyCodec;
set hive.execution.engine=tez;
```

### 总结
通过动态设置分片大小、Map/Reduce数据量以及启用中间压缩，可以显著优化Hive作业的性能。根据实际场景调整这些参数，能够有效提升作业效率并减少资源消耗。



如果您无法通过 `SET` 命令直接设置这些参数，可能是因为这些参数被定义为**静态参数**（static parameters），它们只能在配置文件中修改。以下是解决方法的详细步骤：

---

### **解决方案**

#### 1. **在 `hive-site.xml` 中配置**
您需要修改 Hive 的 `hive-site.xml` 配置文件来设置这些参数。以下是需要添加的内容：

```xml
<property>
  <name>mapreduce.input.fileinputformat.split.maxsize</name>
  <value>256000000</value> <!-- 设置为256MB -->
</property>
<property>
  <name>mapreduce.input.fileinputformat.split.minsize</name>
  <value>128000000</value> <!-- 设置为128MB -->
</property>
<property>
  <name>hive.input.format</name>
  <value>org.apache.hadoop.hive.ql.io.CombineHiveInputFormat</value> <!-- 使用CombineHiveInputFormat -->
</property>


<property>
  <name>mapreduce.input.fileinputformat.split.maxsize</name>
  <value>1024000000</value> <!-- 设置为256MB -->
</property>
<property>
  <name>mapreduce.input.fileinputformat.split.minsize</name>
  <value>512000000</value> <!-- 设置为128MB -->
</property>

```

#### **步骤：**
1. 找到 Hive 的配置文件目录，通常在路径 `/etc/hive/conf/` 或类似目录。
2. 打开或编辑 `hive-site.xml` 文件。
3. 将上述配置添加到文件中。
4. 保存并重新启动 Hive 服务，使配置生效。

---

#### 2. **在启动 Hive 时通过 `hive --hiveconf` 指定**
如果您不方便修改配置文件，可以在启动 Hive CLI 或 Beeline 时，通过 `--hiveconf` 参数临时设置这些参数：

```bash
hive --hiveconf mapreduce.input.fileinputformat.split.maxsize=256000000 \
     --hiveconf mapreduce.input.fileinputformat.split.minsize=128000000 \
     --hiveconf hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat
```

这种方法的设置仅对当前会话有效。

---

#### 3. **修改 Hadoop 的配置文件**
部分参数（如 `mapreduce.input.fileinputformat.split.maxsize` 和 `mapreduce.input.fileinputformat.split.minsize`）属于 Hadoop 的静态参数，可以在 Hadoop 的配置文件中修改。

##### **修改 `mapred-site.xml`**
在 Hadoop 的 `mapred-site.xml` 文件中添加以下内容：

```xml
<property>
  <name>mapreduce.input.fileinputformat.split.maxsize</name>
  <value>256000000</value> <!-- 设置为256MB -->
</property>
<property>
  <name>mapreduce.input.fileinputformat.split.minsize</name>
  <value>128000000</value> <!-- 设置为128MB -->
</property>
```

##### **步骤：**
1. 找到 Hadoop 的配置文件目录，通常在 `/etc/hadoop/conf/`。
2. 打开或编辑 `mapred-site.xml` 文件。
3. 将上述配置添加到文件中。
4. 保存并重新启动 Hadoop 服务，使配置生效。

---

#### 4. **检查 Hive 是否允许设置这些参数**
可以通过 Hive 的 `hive.security.authorization` 配置检查是否允许动态修改参数：

1. 查看当前的 Hive 参数是否被限制：
   ```sql
   set hive.security.authorization.enabled;
   ```
   如果返回 `true`，则动态设置参数被限制。

2. 如果需要允许动态修改，编辑 `hive-site.xml` 文件，设置：
   ```xml
   <property>
     <name>hive.security.authorization.enabled</name>
     <value>false</value>
   </property>
   ```

3. 保存并重新启动 Hive 服务。

---

### **验证设置**
无论使用哪种方法，都可以通过以下命令验证设置是否生效：
```sql
set mapreduce.input.fileinputformat.split.maxsize;
set mapreduce.input.fileinputformat.split.minsize;
set hive.input.format;
```

如果返回的值符合您设置的配置，则说明设置成功。

---

### **注意事项**
- **静态参数**：只能通过配置文件修改，无法通过 `SET` 命令在运行时修改。
- **动态参数**：可以通过 `SET` 命令临时修改，但可能受权限限制。
- 修改配置文件后，需要重启相关服务（Hive 或 Hadoop）以使更改生效。

如果您依然无法修改，请确认是否有权限访问和修改配置文件。



# Spark集群优化

参考链接：

[hive on spark调优点](https://blog.csdn.net/rlnLo2pNEfx9c/article/details/81572016)

[Tuning Apache Hive on Spark in CDH](https://docs.cloudera.com/documentation/enterprise/6/6.3/topics/admin_hos_tuning.html)

[hive on spark调优笔记](https://geekdaxue.co/read/shangtianxiadixiaopg@hyq5oe/ya2x1g)

spark-defaults.conf配置

```bash
# Spark 作业调度模式
spark.master                     yarn
spark.eventLog.enabled           true
spark.eventLog.dir               hdfs://cesdb:8020/user/spark/spark-logs

# 执行器配置
spark.executor.memory            14g               # 每个执行器使用 14GB 内存
# 内存开销配置
spark.yarn.executor.memoryOverhead   2g           # 为每个执行器配置 2GB 内存开销
spark.executor.cores             8                 # 每个执行器使用 8 个核心
# spark.executor.instances         35                # 总共 35 个执行器（5 台机器，每台 7 个执行器）静态分配，不符合生产实际

# 动态资源分配（符合实际生产环境）
# 启动动态分配
spark.dynamicAllocation.enabled  true 
# 启用Spark shuffle服务
spark.shuffle.service.enabled     true
# Executor个数初始值
spark.dynamicAllocation.initialExecutors 1
spark.dynamicAllocation.minExecutors 1
# Executor个数最大值
spark.dynamicAllocation.maxExecutors 35
# Executor空闲时长，若某个Executor空闲时间超过此值，则会被关闭
spark.dynamicAllocation.executorIdleTimeout 60s
# 积压任务等待时长，若有Task等待时间超过此致，则申请启动新的Executor,设置1s太敏感
spark.dynamicAllocation.schedulerBacklogTimeout 3s
# Driver 配置
spark.driver.memory              12g               # Driver 内存，设置为 10GB
spark.driver.memoryOverhead      2g
# 并行度配置
# spark.default.parallelism        280               # 总核心数：5 台机器 * 56 核 = 280 核
# spark.sql.shuffle.partitions    280               # 设置为合理的分区数，确保任务并行执行
```

